{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics\n",
    "In the first part of the lab we study linear averaging dynamics on graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G=(V,E,W)$ be a weighted graph, and $x(t) \\in \\mathrm{R}^{V}$ denote the state of the nodes of the graph.\n",
    "\n",
    "The dynamics of $x(t)$ reads\n",
    "\n",
    "$$\n",
    "x(t+1) = Px(t),\n",
    "$$\n",
    "\n",
    "where $P$ is the normalized adjacency matrix.\n",
    "Among the applications, the most popular is opinion dynamics, where $x_i$ indicates the opinion of node $i$. This dynamics is known as French - De Groot.\n",
    "\n",
    "Note that we assume by convention that the opinion of node $i$ is influenced by the opinion of node $j$ if $P_{ij}>0$, i.e., the link $(i,j)$ has to be interpreted as $i$ watching $j$ and updating her opinion based on opinion of $j$.\n",
    "\n",
    "**Observation**: observe that $\\mathbf{1}$ is an equilibrium distribution, since $\\mathbf{1} = P \\mathbf{1}$ ($P$ is row-stochastic by construction), i.e., consensus distributions are equilibria of the dynamics.\n",
    "\n",
    "**Questions**: \n",
    "- what are the conditions under which the dynamics converges to an equilibrium?\n",
    "- what are the conditions under which all equilibria are consensus configurations?\n",
    "\n",
    "**Theorem**: assume that\n",
    "- its condensation graph has 1 sink;\n",
    "- the graph is aperiodic.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to +\\infty} x(t) = \\alpha \\mathbf{1},\n",
    "$$\n",
    "\n",
    "i.e., the agents get to consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisdom of crowds\n",
    "Consider a graph, and assume that state of each node represents a noisy estimate of the real state $\\mu$, i.e.,\n",
    "\n",
    "$$\n",
    "x_i = \\mu + y_i,\n",
    "$$\n",
    "\n",
    "with $E[y_i]=0$, and the variance $\\sigma^2 (y_i) = \\sigma^2$ for each $i$.\n",
    "\n",
    "Assume that the graph is connected and aperiodic Eventually, the agents will reach consensus, i.e., $\\lim_{t \\to +\\infty} x(t) = \\alpha \\mathbf{1}$. \n",
    "\n",
    "**Question**: what is the consensus value $\\alpha$?\n",
    "\n",
    "Notice that $\\alpha = \\pi' (\\mu \\mathbf{1} + y) = \\mu + \\pi'y$, then\n",
    "\n",
    "$$\n",
    "E[\\alpha] = \\mu + \\pi' E[y] = \\mu\n",
    "$$\n",
    "\n",
    "so the final estimate of the network is unbiased. Moreover,\n",
    "\n",
    "$$\n",
    "\\quad \\sigma_{\\alpha}^2 = \\sigma^2 \\sum_{i} \\pi_i^2 < \\sigma^2,\n",
    "$$\n",
    "\n",
    "because $\\sum_{i} \\pi_i^2 <1$ unless the graph has a unique sink node. The interesting observation is that the estimate $\\alpha$ has a smaller variance than $\\sigma$, i.e., the crowd is able to reconstruct a more precise estimate of the real state than the single agents of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify this on a complete graph, where $\\pi_i = 1/n$, thus $\\sigma_\\alpha^2 = \\sigma^2/n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "G = nx.complete_graph(100)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# start with random initial states and run the dynamics 200 times\n",
    "# store in alfa_err the consensus values at each run\n",
    "alfa_err = np.zeros(200)\n",
    "\n",
    "for i in range(200):\n",
    "# rand returns uniformly random values in [0,1], thus mu = 1/2\n",
    "    x = np.random.rand(100)\n",
    "    for n in range(500):\n",
    "        x = P @ x\n",
    "    alfa_err[i] = (1/2 - np.mean(x))*(1/2 - np.mean(x))\n",
    "\n",
    "print(\"Variance of the node states:\", 1/12)\n",
    "print(\"Variance of the consensus state:\", np.mean(alfa_err), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the variance of $\\alpha$ is about $1/100$ of the original variance.\n",
    "\n",
    "Note that $\\sum_{i} \\pi_i^2$ tends to 1 if one node has almost all the centrality, and is minimal when the nodes have the same centrality (as in the complete graph, or in the cycle graph). This implies that if a graph is more democratic, then the consensus algorithm leads to better estimates of the true state. If a few nodes have all the centralities, the consensus value is less reliable.\n",
    "\n",
    "Let us see this with a another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(10)\n",
    "G = nx.Graph.to_directed(G)\n",
    "G.remove_edges_from([(0,1),(0,9)])\n",
    "G.add_edge(0,0)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with random initial states and run the dynamics\n",
    "alfa_err = np.zeros(200)\n",
    "\n",
    "for i in range(200):\n",
    "# rand returns random values in [0,1], thus \\mu = 1/2\n",
    "    x = np.random.rand(10)\n",
    "    var = np.var(x)\n",
    "    for n in range(500):\n",
    "        x = P @ x\n",
    "    alfa_err[i] = (1/2 - np.mean(x))*(1/2 - np.mean(x))\n",
    "\n",
    "print(\"Expected variance of the node states:\", 1/12)\n",
    "print(\"Empirical variance of the consensus state:\", np.mean(alfa_err), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph the consensus value is exactly the initial state of node $0$, because the condensation graph has 1 sink only, which is node $0$.\n",
    "\n",
    "$$\n",
    "\\pi = \\delta^{(0)}.\n",
    "$$\n",
    "\n",
    "This graph is the opposite of the complete graph, in the sense that the invariant distribution centrality is totally on node $0$. Thus the variance of the consensus state equals the variance of the single node.\n",
    "\n",
    "Note that this argument holds independently of the size of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: distributed computation of average\n",
    "\n",
    "Let the node set describe a set of sensors that are deployed in some regions in order to collect measurements of some quantity of interest (for example, the temperature). \n",
    "\n",
    "Assume that these sensors have limited communication and computation capabilities that allow each of them to exchange information only with those other sensors that are close enough in space. \n",
    "\n",
    "Let the graph $G = (V, E)$ describe the pattern of closeness among the sensors $i$ and $j$ so that there is an undirected link between node $i$ and node $j$ if they can communicate to each other (possibly using link weights decreasing with distance). Then, one can design a distributed algorithm for computing the average of the sensor's measurements based on the averaging dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i(0)$ be the measurement of each node. \n",
    "\n",
    "We are interested in designing an iterative distributed algorithm that allows the nodes to compute\n",
    "\n",
    "$$\n",
    "x = \\frac{1}{n}\\sum_i x_i(0)\n",
    "$$\n",
    "\n",
    "**First attempt**: we run a consensus algorithm. Since the graph is undirected, $\\pi_i = \\frac{w_i}{\\sum_j w_j}$. Thus, the algorithm converges to a consensus $\\alpha \\mathbf{1}$ such that\n",
    "\n",
    "$$\n",
    "\\alpha = \\sum_i \\pi_i x_i(0) = \\sum_{i} \\frac{w_i}{w} x_i(0).\n",
    "$$\n",
    "\n",
    "If each edge knows its degree $w_i$, each node can rescale its initial state, i.e., $y_i(0) = \\frac{x_i(0)}{w_i}$. The consensus algorithm for the variable $y_i$ thus converges to\n",
    "\n",
    "$$\n",
    "\\alpha_y = \\sum_{i} \\frac{w_i}{w} y_i(0) = \\frac{1}{w} \\sum_{i} x_i(0).\n",
    "$$\n",
    "\n",
    "If we assume that each node knows the average degree of the network, thus\n",
    "\n",
    "$$\n",
    "x = \\alpha_y \\frac{w}{n} = \\alpha_y \\overline{w}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Fix node positions on all pictures according to spring layout\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw_networkx(G, pos)\n",
    "\n",
    "n_nodes = len(G)\n",
    "\n",
    "x = np.random.rand(n_nodes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us run the consensus algorithm for y\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "y = x/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    y = P @ y\n",
    "\n",
    "print(\"Average state:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus on y\n",
    "print(\"Average computed distributively\", y[0] * np.sum(degrees) / n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works! Unfortunately, requiring that each node knows the average degree of the network is not realistic (and not distributed).\n",
    "\n",
    "However, there exists another way to solve the problem.\n",
    "\n",
    "**Second attempt**: we run a second averaging dynamics, with initial condition $z_i(0) = \\frac{1}{w_i}$.\n",
    "\n",
    "This converges to\n",
    "\n",
    "$$\n",
    "\\alpha_z = \\lim_{t \\to + \\infty} z_i(t) = \\sum_i z_i(0) \\frac{w_i}{w} = \\sum_{i} \\frac{1}{w} = \\frac{1}{\\overline{w}}\n",
    "$$\n",
    "\n",
    "By combining the two, each node can estimate the average estimate by \n",
    "\n",
    "$$\n",
    "\\frac{\\lim_{t \\to + \\infty} y_i(t)}{\\lim_{t \\to + \\infty} z_i(t)} = \\frac{\\alpha_y}{\\alpha_z} = \\frac{x}{\\overline{w}} \\cdot \\overline{w} = x.\n",
    "$$\n",
    "\n",
    "This method does not require any global knowledge on the network and it is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us implement this\n",
    "z = 1/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    z = P @ z\n",
    "\n",
    "print(\"Average state:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus both on y and z\n",
    "print(\"Average computed distributively\", y[0] / z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics with stubborn agents\n",
    "We study how to simulate the linear averaging dynamics on graphs in presence of stubborn agents.\n",
    "\n",
    "We focus on the optimal placement problem, which consists of optimally chosing the position of a stubborn node on the graph in order to maximize its influence on the asymptotic outcome of the dynamics.\n",
    "\n",
    "Let us first summarize the theory in presence of stubborn agents.\n",
    "\n",
    "We are given a network, where the agents $V$ are divided in regular agents $R$ and stubborn agents $S$. The regular agents update their opinion $x(t)$ according to the standard DeGroot model, while the stubborn agents do not update their opinions, so that $u(t) \\equiv u$.\n",
    "\n",
    "Let $Q=P|_{R \\times R}$ and $E=P|_{R \\times S}$.\n",
    "\n",
    "Thus, the dynamics for the regular agents read\n",
    "\n",
    "$$\n",
    "x(t+1) = Qx(t) + Eu.\n",
    "$$\n",
    "\n",
    "Under some assumptions (see the lecture notes for details) the dynamics converges to \n",
    "\n",
    "$$\n",
    "x^* = (\\mathbf{I}-Q)^{-1}Eu.\n",
    "$$\n",
    "\n",
    "Note that, in contrast with the dynamics without stubborn nodes:\n",
    "- the asymptotic state is not a consensus;\n",
    "- the asymptotic state does not depend on the initial opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We start by implementing the averaging dynamics with stubborn nodes. \n",
    "\n",
    "To illustrate this procedure, we will analyse the following example that involves a $3 \\times 4$ grid graph $\\mathcal G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.generators.lattice.grid_graph(dim=[3,4])\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw_spectral(G, with_labels=True)\n",
    "\n",
    "print(G.nodes())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary that maps the label of nodes  \n",
    "# (from (0,0) to (2,1)) to their index (from 0 to n_nodes-1)\n",
    "indices = dict()\n",
    "for i in range(n_nodes):\n",
    "    indices[list(G.nodes)[i]] = i\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "    \n",
    "# Stubborn and regular nodes\n",
    "stubborn = [(0,0), (3,2)];\n",
    "stubborn_id = [indices.get(key) for key in stubborn]\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to stubborn nodes\n",
    "u = [0,1]\n",
    "\n",
    "# P matrix\n",
    "A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Submatrices\n",
    "# Using ix_ one can construct index arrays that will \n",
    "# index a cross product. \n",
    "# a[np.ix_([1,3],[2,5])] returns the array [[a[1,2] a[1,5]], [a[3,2] a[3,5]]].\n",
    "Q = P[np.ix_(regular_id, regular_id)]\n",
    "E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "# Sample a random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.average(x_final)\n",
    "print(\"Average asymptotic opinion:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dynamics does not converge to consensus. Moreover, in contrast with averaging without input stubborn nodes, we can verify that the asymptotic equilibrium does not depend on the initial condition. Furthermore, the dynamics converge to an equilibrium even though the graph is periodic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample another random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])\n",
    "    \n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal placement of stubborn nodes\n",
    "Suppose that node $(0,0)$ is stubborn with opinion $u_{(0,0)}=0$. We want to find the optimal position $(i,j)$ of a stubborn node with opinion $1$ in order to maximize the asymptotic average opinion.\n",
    "\n",
    "A very simple approach is to consider all possible positions $(i,j)$ and pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "\n",
    "# We will store final opinion vectors and \n",
    "# average of final opinions in dictionaries\n",
    "# where the key is the position (i,j) of the \n",
    "# 1-stubborn agent\n",
    "final_opinions = dict()\n",
    "average_opinion = dict() \n",
    "\n",
    "\n",
    "for (i,j) in G.nodes:\n",
    "    # Position (0,0) is occupied by the 0-stubborn node\n",
    "    if (i,j)==(0,0):\n",
    "        continue\n",
    "        \n",
    "    # Stubborn and regular nodes\n",
    "    stubborn = [(0,0), (i,j)];\n",
    "    stubborn_id = [indices.get(key) for key in stubborn]\n",
    "    regular = [node for node in G.nodes if node not in stubborn]\n",
    "    regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "    print(\"Stubborn nodes:\", stubborn)\n",
    "\n",
    "    # Input to stubborn nodes\n",
    "    u = [0,1]\n",
    "\n",
    "\n",
    "    # P matrix\n",
    "    A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "    A = A.toarray() # convert A to a numpy array\n",
    "    degrees = np.sum(A,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ A\n",
    "\n",
    "    # Submatrices\n",
    "    Q = P[np.ix_(regular_id, regular_id)]\n",
    "    E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "    # Sample a random initial condition for regular nodes\n",
    "    ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "    # Set the initial condition for the dynamics\n",
    "    x = np.zeros((n_nodes,n_iter))\n",
    "    x[stubborn_id,0] = u;\n",
    "    x[regular_id,0] = ic;\n",
    "\n",
    "    for t in range(1,n_iter):\n",
    "        x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "        x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "    final_opinions[(i,j)] = x[:,n_iter-1]\n",
    "    average_opinion[(i,j)] = np.average(final_opinions[(i,j)])\n",
    "    print(\"Average opinion:\", average_opinion[(i,j)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the dependence of the average asymptotic opinion on the position of the $1$-stubborn node we can plot the grid graph by setting each node's size and color according to the magnitude of the average asymptotic opinion when the $1$-stubborn is placed in such node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy (0,0) entry to the dictionary\n",
    "# to make its size = n_nodes\n",
    "average_opinion[(0,0)] = 0\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(10*average_opinion[node]) for node in G.nodes],\n",
    "        node_color= [average_opinion[node] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal placements of the 1-stubborn player are the maximizers of the final average opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the average opinion values from dict_values to numpy array\n",
    "avg = np.fromiter(average_opinion.values(),dtype=float)\n",
    "\n",
    "optimal_place = [place for place in average_opinion.keys() if average_opinion[place]==np.max(avg)]\n",
    "print(\"Optimal placements:\", optimal_place, \"\\n\")\n",
    "\n",
    "# print the final opinions under optimal placement\n",
    "opt_final = final_opinions.get(*optimal_place)\n",
    "print(\"Final opinions after optimal placement:\", opt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the asymptotic opinions of the nodes when the stubborn is placed in (1,1)\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(8*opt_final[indices.get(node)]) for node in G.nodes],\n",
    "        node_color= [opt_final[indices.get(node)] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to an old example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(10)\n",
    "G = nx.Graph.to_directed(G)\n",
    "G.remove_edges_from([(0,1),(0,9)])\n",
    "G.add_edge(0,0)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "n_iter = 100\n",
    "x = np.zeros((10,n_iter))\n",
    "\n",
    "# set initial condition (1,0,0,0,0,0,0,0,0,0)\n",
    "x[0,0] = 1\n",
    "\n",
    "# evolve the states\n",
    "for t in range(1,n_iter):\n",
    "    x[:,t] = P @ x[:,t-1]\n",
    "\n",
    "print(\"Average final opinions:\", np.mean(x[:,n_iter-1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prove that this is equivalent to having node 0 stubborn with opinion 1.\n",
    "\n",
    "Indeed, note that because of the topology of the graph, the opinion of node 0 is not influenced by anyone. The same dynamics can be obtained by assuming that node 0 is stubborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stubborn and regular nodes\n",
    "stubborn = [0];\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [1]\n",
    "\n",
    "# Submatrices\n",
    "Q = P[np.ix_(regular, regular)]\n",
    "E = P[np.ix_(regular, stubborn)]\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn,0] = u;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular, t] = Q @ x[regular, t-1] + E @ x[stubborn, t-1]\n",
    "    x[stubborn, t] = x[stubborn, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "print(x_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same trajectory as before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general model: overview on Friedkin-Johnsen model\n",
    "The French-DeGroot dynamics can be generalized by taking into account that each agents is not completely regular or completely stubborn. The opinion of each agents is in part due to \"innate\" opinions, and in part due to influence of society.\n",
    "\n",
    "The following opinion dynamics model is known as Friedkin-Johnsen model.\n",
    "Here:\n",
    "- $x_i(t)$ is the opinion of the agent $i$;\n",
    "- $y_i$ is the innate opinion of agent $i$.\n",
    "- $\\alpha_i$ is its level of stubborness, i.e., the level of confidence in his/her opinion $y_i$.\n",
    "\n",
    "The dynamics reads:\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\alpha_i y_i + (1-\\alpha_i) \\sum_{j} P_{ij} x_j(t).\n",
    "$$\n",
    "\n",
    "If $\\alpha = \\mathbf{0}$, we get the French-DeGroot model without input.\n",
    "\n",
    "If $\\alpha \\in \\{0,1\\}^{V}$, we get the French-DeGroot model with stubborn nodes.\n",
    "\n",
    "As the French-DeGroot model with input, also the Friedkin-Johnsen dynamics converges to a non-consensus state, which depends on $\\alpha, y$, but not on the initial opinions $x(0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example: Random Walks on graphs and flow dynamics\n",
    "In this section we study a first example of discrete time Markov chain, which is the simple random walk on a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Random Walk\n",
    "\n",
    "A random walker on a graph $G$ is an agent that starts at the initial time $0$ at some node and at each time moves from the current position to a neighboring one, chosen with uniform probability (this can be generalized).\n",
    "\n",
    "To learn how to simulate a random walk, here we consider the example of a $n \\times n$ chessboard with a single knight on it. \n",
    "1. We construct a network $G$ with all knight's possible moves. In this network nodes represent chessboard locations and an edge between two locations is present if the knight is admitted to move from one to another.\n",
    "2. We implement a simulation of the knight's random walk on the chessboard network $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice, rand "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Knight's Network\n",
    "\n",
    "Here we define function `GenerateKnightNetwork` that constructs the knight's network. \n",
    "It exploits two auxiliary functions, `ApplyLegalMoves` and `isLegalPos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if the position obtained applying a move is legal,\n",
    "# i.e. if it is inside the chessboard\n",
    "def isLegalPos(x,boardSize):\n",
    "    if x >= 0 and x < boardSize:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Apply the knight's legal moves to the current position to construct\n",
    "# neighboring nodes in the knight's graph\n",
    "\n",
    "def ApplyLegalMoves(x,y,boardSize):\n",
    "    # will store the neighboring nodes\n",
    "    new_positions = []\n",
    "    \n",
    "    # offsets describe the effect of the knight's legal moves\n",
    "    # on the current position's row and column\n",
    "    offsets = [(-1,-2),(-1,2),(-2,-1),(-2,1),\n",
    "                   ( 1,-2),( 1,2),( 2,-1),( 2,1)]\n",
    "    \n",
    "    # for each legal move, compute the new position's row and column\n",
    "    for off in offsets:\n",
    "        new_x = x + off[0]\n",
    "        new_y = y + off[1]\n",
    "        \n",
    "        # if the new position doesn't exceed the boardsize,\n",
    "        # accept it as legal\n",
    "        if isLegalPos(new_x,boardSize) and isLegalPos(new_y,boardSize):\n",
    "            new_positions.append((new_x,new_y))\n",
    "         \n",
    "    return new_positions\n",
    "\n",
    "# Generates the graph representing the knigth network.\n",
    "# Return both the graph object G and the pos dictionary\n",
    "# for drawing G.\n",
    "\n",
    "def GenerateKnightNetwork(boardSize):\n",
    "    # undirected graph G will store the knight's network\n",
    "    G = nx.Graph()\n",
    "    # when drawing G, the pos dictionary describes the position on \n",
    "    # a boardsize x boardsize grid where to place nodes\n",
    "    pos = {}\n",
    "    \n",
    "    # we assign position to nodes\n",
    "    # we have boardSize x boardSize nodes\n",
    "    for row in range(boardSize):\n",
    "        for column in range(boardSize):\n",
    "            node_id = row + column*boardSize\n",
    "            # pos[node_id] are the (x,y) coordinates of node node_id \n",
    "            # on a square grid of side boardSize\n",
    "            pos[node_id] = np.array([1.0*row/boardSize, 1.0*column/boardSize])\n",
    "            \n",
    "            # compute the (row,column) of neighboring position to the\n",
    "            # current one, i.e., positions on the chessoboard reachable\n",
    "            # by applying legal moves\n",
    "            neigh_pos = ApplyLegalMoves(row, column, boardSize)\n",
    "            # for each neigbhoring position, compute the id and add\n",
    "            # a link in G from current position node_id to neigh_id\n",
    "            for p in neigh_pos:\n",
    "                neigh_id = p[0] + p[1]*boardSize\n",
    "                G.add_edge(node_id, neigh_id)\n",
    "    return G, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to generate an example of the knight's network, with a specified boardsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boardSize = 8\n",
    "# use position dictionary returned by the function GenerateKnightNetwork\n",
    "(G,pos) = GenerateKnightNetwork(boardSize)\n",
    "nx.draw(G,pos, with_labels=True, node_color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate the Random Walk Process\n",
    "Now that we have the graph $G$ representing the knight's network, we can simulate the knight's random walk on it. \n",
    "\n",
    "The walk starts at some given node and it can either terminate after a specified number of steps or when it first returns to the starting node.\n",
    "\n",
    "At each step, the walker is at some node `xi` and has to decide which node to visit next. \n",
    "\n",
    "In this simple version of the random walk, he does it by choosing a neighbor of the current node uniformly at random.\n",
    "More formally, he looks at row `xi` of the normalized weight matrix $P$ of the graph $G$ and interprets the numbers on that row as the probability of visiting the corresponding nodes next, given that he currently is at `xi`.\n",
    "\n",
    "To simulate the random walk we define the function `RandomWalk`, which allows to specify the graph $G$ on which the walk takes place, the starting node and the stopping criterion.\n",
    "\n",
    "**Remark 1**: the `RandomWalk` function is independent on the specific example we are studying. In other words, it allows to simulate any simple random walk on any **unweighted** graph $G$, since it only exploits the general features of the stochastic process at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates a random walk on the graph G, starting from node xi.\n",
    "# if till_first_return = True the random walk stops the first time\n",
    "# it returns to the starting node xi.\n",
    "# Otherwise, it goes on for num_steps steps.\n",
    "\n",
    "def RandomWalk(G, xi, num_steps, till_first_return = False):\n",
    "    # nodeSeq stores the sequence of visited nodes\n",
    "    nodeSeq = []\n",
    "    nodeSeq.append(xi)\n",
    "    \n",
    "    # if the walk ends at the first return to xi\n",
    "    if till_first_return:\n",
    "        # stores the initial position to check if the \n",
    "        # walk returns to it\n",
    "        x_init = xi\n",
    "        \n",
    "        # no upper bound on the number of steps\n",
    "        while True:\n",
    "            # compute the next visited node xi by chosing uniformly\n",
    "            # at random a neighbor of the current node\n",
    "            xi = choice(G.adj[xi],1)[0]     \n",
    "            nodeSeq.append(xi)\n",
    "            \n",
    "            # check if the walk has returned to the starting node\n",
    "            # if so, end the walk\n",
    "            if xi == x_init:\n",
    "                return nodeSeq\n",
    "    \n",
    "    # if the walk ends after num_steps steps\n",
    "    else:\n",
    "        for i in range(num_steps):\n",
    "            xi = choice(G.adj[xi],1)[0]      \n",
    "            nodeSeq.append(xi)\n",
    "        return nodeSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement RandomWalk with different stopping criteria, or using non-uniform transition probability distributions (for weighted graphs).\n",
    "\n",
    "As a first experiment, we simulate a simple random walk on $G$ with $10$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G \n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=10, till_first_return=False)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)\n",
    "\n",
    "print(\"Node sequence:\", nodeSeq)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G that stops at the first return time\n",
    "# note that if till_first_return = True, 'num_steps' is negligible\n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=1, till_first_return=True)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)\n",
    "\n",
    "# if the node sequence is not deducible from the plot, you can print the nodeSeq\n",
    "print(\"Node sequence:\", nodeSeq)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a random walk on G with 100 steps\n",
    "nodeSeq = RandomWalk(G, xi=0, num_steps=200, till_first_return=False)\n",
    "edgeSeq = [(nodeSeq[i-1], nodeSeq[i]) for i in range(1,len(nodeSeq))]\n",
    "\n",
    "# Draw G and represent the random walk by colouring the edge sequence\n",
    "# first draw all nodes and links\n",
    "nx.draw(G, pos)\n",
    "# then, on the previous picture, add node labels and highlight the edge sequence\n",
    "nx.draw(G, pos, with_labels=True, edgelist = edgeSeq, edge_color='blue', node_color='red', width=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even with a larger number of steps some of the nodes may not be visited!\n",
    "\n",
    "**Question**: do you think that all the nodes have the same probability of being visited?\n",
    "\n",
    "To answer this question, we need to relate random walks to flow dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walks and Linear Flow Dynamics\n",
    "\n",
    "The random walk and the flow dynamics are deeply connected. Indeed, if we describe the position of the walker on $G$ at time $t$ with a random variable $x(t)$, the variable's probability distribution evolves according to a flow dynamics. Let\n",
    "\n",
    "$$\n",
    "\\pi_i(t) = \\mathbf{P}\\{x(t)=i\\}.\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\pi(t+1) = P'\\pi(t)\n",
    "$$\n",
    "\n",
    "or more explicitly\n",
    "\n",
    "$$\n",
    "\\pi_j(t+1) = \\sum_{i \\in \\mathcal V} P_{ij}\\pi_i(t),\n",
    "$$\n",
    "\n",
    "Moreover, one can use a random walk to estimate the invariant measure $\\pi$ of the graph $G$ (assume here $G$ is strongly connected and aperiodic). Indeed, by the Katz theorem, the fraction of time spent by the walker on each node tends to the node's value in the invariant measure $\\pi$ as the length of the walk increases.\n",
    "\n",
    "The following section shows how to compute empirical frequencies and how to compare them with the inviariant distribution of $G$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical frequencies and invariant distribution\n",
    "The empirical frequencies are the fractions of total walk time that each node of $G$ is visited in the random walk. \n",
    "\n",
    "They can be represented by a histogram as follows.\n",
    "\n",
    "We simulate random walks starting at each node of $G$, and we keep track of the sequence of visited nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodeSeq: list to store all random walks\n",
    "nodeSeq = []\n",
    "\n",
    "# simulate one random walk for each initial node in G\n",
    "for xi in range(G.number_of_nodes()):\n",
    "    # list.extend extends the list by appending all the items from an iterable\n",
    "    nodeSeq.extend(RandomWalk(G, xi, 100, False))\n",
    "    \n",
    "# plt.hist computes and draws the histogram of nodeSeq. We have one bin for each node,\n",
    "# so each bin width is 1 and the number of observations in each bin equals the number\n",
    "# of visits to the correspnding node. \n",
    "# Since density=True, the return element will be the counts normalized \n",
    "# to form a probability density.\n",
    "h = plt.hist(nodeSeq, bins = G.number_of_nodes(), density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use empirical frequencies to approximate the invariant measure of $G$, we have to construct long random walks. As an example, here we run a random walk with $10000$ steps for each initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodeSeq: list to store all random walks\n",
    "nodeSeq = []\n",
    "\n",
    "# simulate one random walk for each initial node in G\n",
    "for xi in range(G.number_of_nodes()):\n",
    "    # list.extend extends the list by appending all the items from an iterable\n",
    "    nodeSeq.extend(RandomWalk(G, xi, 10000, False))\n",
    "    \n",
    "# plt.hist computes and draws the histogram of nodeSeq. We have one bin for each node,\n",
    "# so each bin width is 1 and the number of observations in each bin equals the number\n",
    "# of visits to the correspnding node. \n",
    "# Since density=True, the return element will be the counts normalized \n",
    "# to form a probability density.\n",
    "h = plt.hist(nodeSeq, bins = G.number_of_nodes(), density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequencies are now smoother, and look not uniform, as expected. Let us compare frequencies with the invariant distribution of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a \"long\" random walk\n",
    "nodeSeq = RandomWalk(G, 0, 100000, False)\n",
    "\n",
    "h = plt.hist(nodeSeq, bins = G.number_of_nodes(), density=True)\n",
    "\n",
    "# Compute empirical frequencies\n",
    "\n",
    "frequencies = np.zeros(len(G))\n",
    "# count the visits to each node\n",
    "for node in nodeSeq:\n",
    "    frequencies[node] += 1\n",
    "# normalize the counts to obtain frequencies\n",
    "frequencies /= len(nodeSeq)\n",
    "print(\"Frequencies:\", frequencies, \"\\n\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the nodes to compare empirical frequencies with invariant distribution\n",
    "H = nx.Graph()\n",
    "H.add_nodes_from(sorted(G.nodes(data=True)))\n",
    "H.add_edges_from(G.edges(data=True))\n",
    "\n",
    "print(H.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute P matrix\n",
    "A = nx.adjacency_matrix(H) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Compute invariant distribution\n",
    "values,vectors = np.linalg.eig(P.T)\n",
    "index = np.argmax(values.real)\n",
    "pi = vectors[:,index].real\n",
    "pi = pi/np.sum(pi)\n",
    "\n",
    "print(\"pi=\", pi, \"\\n\")\n",
    "\n",
    "print(\"frequencies=\", frequencies, \"\\n\")\n",
    "\n",
    "# Evaluate the approximation error by computing the norm of\n",
    "# the difference between the empirical frequencies and the \n",
    "# invariant measure\n",
    "error = np.linalg.norm(frequencies-pi)\n",
    "print(\"Error\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Note how important the invariant distribution is. So far, it appeared in many different contexts:\n",
    "- centrality measures in social networks;\n",
    "- consensus value in averaging dynamics;\n",
    "- asymptotic limit of linear flow dynamics;\n",
    "- invariant probability distributions in random walks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Consider the graph shown in the following picture:\n",
    "![Exgraph](graph1.png)\n",
    "\n",
    "**Remark 2**: for nodes without out-going links, follow the convention and add a self-loop.\n",
    "\n",
    "1. For each node of $G$, simulate the simple random walk starting from that node. What do you observe? How can you justify your observations?\n",
    "2. Study the graph $G$ and compute its invariant measures. Can you see any relation with the behavior of the random walk?\n",
    "3. Substitute the link (5,1) with the link (1,5) and repeat the analysis performed at point 1. and 2. What topological property of the graph has changed? How does this reflect on the random walk process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete time Markov chains\n",
    "We already saw an example of discrete time Markov chain: the random walk on the knight's network.\n",
    "\n",
    "In general, every discrete time Markov chain can be interpreted as a random walk on a weighted directed graph. So, in the general case, transition probabilities from each node to its neighbors are not uniformly distributed.\n",
    "\n",
    "To better understand this notion, we analyse the following example.\n",
    "\n",
    "![DMCgraph](discreteMC.png)\n",
    "\n",
    "1. Construct the directed graph $G$ with weights as shown in the picture.\n",
    "2. Compute the invariant probability distribution vector by computing the leading eigenvector of $P'$.\n",
    "3. Simulate a random walk starting from node 1 on the graph for n = 1000, 2000, 5000, 10000 steps. Determine the fraction of the steps the walk has been in each node i. Compare this with the invariant distribution. What do you observe?\n",
    "\n",
    "**Hint:** you can adopt the function RandomWalk by modifying the choice of the next node to visit according to the fact that the transition probability is not uniform.\n",
    "\n",
    "4. What happens with your estimate of $\\pi$ if you remove node 5 and all links connected to it from the graph and add a self loop of weight 1 to node 6?\n",
    "5. Compute the expected hitting time $\\mathbb{E}_j[T_S]$, $\\forall j \\in R = \\mathcal V \\setminus \\mathcal S$, for the set $S = \\{2, 5\\}$ analytically.\n",
    "6. For every node i, simulate several times a random walk on $G$ that begins in node i and stops when it comes back to it. Use this simulation to estimate the expected return time, $\\mathbb{E}_i[T_i^+]$. Compare this estimate with the expected return times obtained analytically from the expected hitting times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "# add weighted edges\n",
    "G.add_weighted_edges_from([(1,3,1),(1,4,2),(2,1,1),(3,1,7),(3,6,3),(4,2,1),(4,3,4),(5,5,2),(5,3,1),(6,5,1)])\n",
    "\n",
    "# define a new graph with sorted nodes\n",
    "H = nx.DiGraph()\n",
    "H.add_nodes_from(sorted(G.nodes(data=True)))\n",
    "H.add_edges_from(G.edges(data=True))\n",
    "\n",
    "nx.draw(H,with_labels=True)\n",
    "\n",
    "# Compute P matrix\n",
    "A = nx.adjacency_matrix(H) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "n_nodes = H.number_of_nodes()\n",
    "\n",
    "print(\"P:\", P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. If node 5 is removed, node 6 is a sink node. Hence, all random walks will eventually reach this node so that the estimate of the invariant measure will converge to $\\pi =[0, 0, 0, 0, 1]$. You can check this by simulating the random walk on the modified graph.\n",
    "\n",
    "The interpretation for this is that as the random walks hits 6, it cannot escape. Thus, from there on, it will stay in 6 forever. In the limit of infinite $t$, the fraction of time spent in node 6 tends to 1. This is consistent with the fact that invariant distribution centrality in a graph has support only on nodes that belong to trapping set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The expected hitting times  $\\hat{x}= (\\mathbb{E}_i[T_S])_{i \\in R}$ for the set $S$ and for all nodes $i \\in R = V \\setminus S$ can be computed by solving the system of equations\n",
    "\n",
    "$$\n",
    "\\hat{x} = \\mathbf{1} + \\hat{P}\\hat{x},\n",
    "$$ \n",
    "\n",
    "where $\\hat{P}$ is obtained from $P$ (the normalized weight matrix of the graph) by removing the rows and columns corresponding to the nodes in the set $S$.\n",
    "\n",
    "More explicitly, the expected hitting times can be expressed as\n",
    "\n",
    "$$\n",
    "\\hat{x} = (I - \\hat{P})^{-1} \\mathbf{1}\n",
    "$$\n",
    "\n",
    "**Remark**: note that $(I - \\hat{P})$ is invertible only if $V \\setminus S$ has at least a link pointing to $S$. Indeed, if $(I - \\hat{P})$ is not invertible, the random walk starting from nodes in $V \\setminus S$ cannot hit nodes in $S$, and the hitting times diverge.\n",
    "\n",
    "Thus, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the set S and the remaining nodes R\n",
    "# Subtract -1 because indexes go from 0 to 5 and nodes from 1 to 6\n",
    "S = [1, 4] # refer to nodes [2,5]\n",
    "R = [node for node in range(n_nodes) if node not in S]\n",
    "\n",
    "# Restrict P to R x R to obtain hat(P)\n",
    "hatP = P[np.ix_(R, R)]\n",
    "\n",
    "# solve the linear system to obtain hat(x)\n",
    "# np.linalg.solve solves a linear matrix equation given\n",
    "# the coefficient matrix and the dependent variable values\n",
    "hatx = np.linalg.solve((np.identity(n_nodes-2)-hatP),np.ones(n_nodes-2))\n",
    "# map node to position of node in hatx\n",
    "map = {0: 0, 2: 1, 3: 2, 5: 3}\n",
    "\n",
    "# define the hitting times to the set S\n",
    "# hitting time is 0 if the starting node is in S\n",
    "hitting_s = np.zeros(n_nodes)\n",
    "# hitting time is hat(x) for nodes in R\n",
    "for r in R:\n",
    "    hitting_s[r] = hatx[map[r]]\n",
    "\n",
    "print(\"hitting times:\", hitting_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for node 6 the expected hitting time is 1, because its only out-neighbour belongs to $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. To compute expected return times analytically, recall that they can be caracterized by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_i[T_i^+] = 1 + \\sum_{j} P_{ij} \\mathbb{E}_j[T_i]\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}_j[T_i]$ is the expected hitting time to the set $S={i}$ starting from $j$.\n",
    "\n",
    "So, for computing $\\mathbb{E}_i[T_i^+]$, one can:\n",
    "- set $S=\\{i\\}$\n",
    "- compute the expected hitting time to $S$, $$\\mathbb{E}_j[T_i], \\quad \\forall j \\in V\\setminus \\{i\\}$$ (as done in point 5)\n",
    "- apply the linear relation $\\mathbb{E}_i[T_i^+] = 1 + \\sum_{j} P_{ij} \\mathbb{E}_j[T_i]$\n",
    "\n",
    "Instead, an estimation of the expected return time $\\mathbb{E}_i[T_i^+]$, $\\forall i$, is obtained by simulating random walks that start at $i$ and end at the first return (you can use `RandomWalk(G, xi=i, num_steps = 'inf', till_first_return = True)`)\n",
    "\n",
    "Implement and compare the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Time Markov Chains\n",
    "\n",
    "In CTMC, the time is not discrete ($t=0,1,\\ldots$) but it flows in a continuum ($ t \\geq 0$).\n",
    "\n",
    "The random process still describes the evolution of a state variable $x$ inside a discrete state space $\\mathcal X$ with a graph structure.\n",
    "We are given a graph $G =(\\mathcal X, \\Lambda)$ with nodes $\\mathcal X$ and weight matrix $\\Lambda$ describing possible transitions between nodes/states.\n",
    "\n",
    "Transitions now happen at random time instants that are decided by the tick of a so called **Poisson clock**. A Poisson clock is characterized by the property that the time elapsed between any two of\n",
    "its consecutive ticks is an independent random variable with exponential distribution with a specified rate.\n",
    "\n",
    "**Remark 1**:\n",
    "to simulate continuous time Markov chains the following fact will be useful.\n",
    "To simulate a Poisson clock with rate $r$, one must simulate the time between two consecutive ticks, which we denote by $t_{next}$. We can compute $t_{next}$ as\n",
    "\n",
    "$$ t_{next} = - \\frac{\\ln(u)}{r}$$\n",
    "\n",
    "where $u$ is a random variable with uniform distribution, $u \\in \\mathcal{U}(0,1)$.\n",
    "\n",
    "\n",
    "### Modelling Continuous time Markov chains\n",
    "There are three equivalent ways of modelling CTMCs.\n",
    "\n",
    "**1st approach**\n",
    "1. you define a unique **global** Poisson clock with an appropriate rate $\\omega^* = \\max_i(\\omega_i)$ where $\\omega_i= \\sum_j \\Lambda_{ij}$\n",
    "2. when you are at node $i$ and **the global clock ticks**, either you jump to a neighbor $j$ with probability $\\bar P_{ij} = \\frac{\\Lambda_{ij}}{\\omega_{*}}, \\; i \\neq j$ or you stay in the same node (no transition) with probability $\\bar P_{ii} = 1 - \\sum_{i \\neq j} \\bar P_{ij}$.\n",
    "\n",
    "In this approach, the continuous time is discretized using a global clock, while the matrix $\\bar P$ describes the jumps. For this reason the matrix $\\bar P$ is called **jump chain** of the CTMC.\n",
    "\n",
    "Notice that $\\bar P_{ii}=0$ for the nodes $i$ maximizing $\\omega$, and it is larger as $\\omega_i/\\omega$ is small.\n",
    "\n",
    "**2nd approach**\n",
    "1. each node $i$ is equipped with its own Poisson clock with rate $\\omega_i= \\sum_j \\Lambda_{ij}$.\n",
    "2. when you are at node $i$ and **the clock of that node ticks**, you jump to a neighbor $j$ with probability  $P_{ij} = \\frac{\\Lambda_{ij}}{\\omega_i}$.\n",
    "\n",
    "**3rd approach**\n",
    "1. each link $(i,j)$ is equipped with a Poisson clock with rate $\\omega_{(i,j)} = \\Lambda_{ij}$. \n",
    "2. when the clock of link $(i,j)$ ticks and the Markov chain is in $i$, it moves to $j$.\n",
    "\n",
    "For nodes $i$ such that $\\omega_i=0$ (which means that once the process in $i$ it remains $i$ forever), we need to add a selfloop $\\Lambda_{ii}>0$, otherwise the matrix $P$ is not well defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Suppose that the weather can be modelled as a continuous-time Markov chain, with state space $\\mathcal{X} = \\{sunny,rainy, cloudy,snowy\\}$. Let the transition rates be\n",
    "\n",
    "![transitionRates](transitionMatrix.png)\n",
    "\n",
    "1. The probability distribution $\\bar{\\pi}(t)$ of the CTMC $X(t)$ with transition rate matrix $\\Lambda$ is defined as\n",
    "\n",
    "$$\n",
    "\\bar{\\pi}_i(t) = \\mathbb P(X(t) = i), \\quad i \\in \\mathcal X \\,.\n",
    "$$\n",
    "\n",
    "It evolves according to the equation\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\bar{\\pi}(t) = -L'\\bar{\\pi}(t)\n",
    "$$\n",
    "\n",
    "where $L= diag(w) - \\Lambda$, with $w = \\Lambda \\mathbf{1}$.\n",
    "\n",
    "Thus, the invariant probability vectors are eigenvector of $L'$ corresponding to eigenvalue $0$. It can also be proven that $\\bar{\\pi}$ is the left dominant eigenvector of $\\bar P$, where $\\bar P$ is the row-stochastic matrix defined as\n",
    "\n",
    "$$\n",
    "\\bar P_{ij} = \\frac{\\Lambda_{ij}}{\\omega_{*}}, \\; i \\neq j \\quad \\bar P_{ii} = 1 - \\sum_{i \\neq j} \\bar P_{ij}\n",
    "$$\n",
    "\n",
    "with $\\omega = \\Lambda \\mathbf{1}$ and $\\omega_{*}=\\max_i \\omega_i$.\n",
    "\n",
    "Compute the invariant probability vector $\\bar{\\pi}$ of the CTMC by determining the leading eigenvector of the matrix $\\bar P'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = [\n",
    "[0, 1/30, 1/15, 1/60],\n",
    "[1/60, 0, 1/10, 1/100],\n",
    "[1/25, 1/10, 0, 1/50],\n",
    "[1/100, 1/10, 1/10, 0]]\n",
    "\n",
    "w = np.sum(Lambda, axis=1)\n",
    "w_star = np.max(w)\n",
    "# compute the off-diagonal part of P_bar\n",
    "P_bar = Lambda/w_star \n",
    "# add the diagonal part\n",
    "P_bar = P_bar + np.diag(np.ones(len(w))-np.sum(P_bar,axis=1))\n",
    "\n",
    "# compute dominant eigenvector\n",
    "values,vectors = np.linalg.eig(P_bar.T)\n",
    "index = np.argmax(values.real)\n",
    "pi_bar = vectors[:,index].real\n",
    "pi_bar = pi_bar/np.sum(pi_bar)\n",
    "print(\"pi_bar=\", pi_bar)\n",
    "\n",
    "nstates = len(pi_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Simulate the continuous-time Markov chain starting from sunny weather. Do this following two different approaches:\n",
    "\n",
    "**a)** 1st approach, i.e., using global clock with rate $\\omega^* = \\max_i{\\omega_i}$ and the conditional probability matrix $\\bar P$.\n",
    "\n",
    "**b)** 2nd approach, i.e., using a rate-$\\omega_i$ clock in each node $i$ and the conditional probability matrix $P$.\n",
    "\n",
    "In both cases:\n",
    "- plot the trajectory for the first 20 jumps,\n",
    "- use the simulation to estimate $\\bar{\\pi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st approach: global clock with rate w_star and matrix P_bar\n",
    "\n",
    "# set the number of steps in the simulation\n",
    "n_steps = 10000\n",
    "# pos will keep trace of the visited states\n",
    "pos = np.zeros(n_steps, dtype=int)\n",
    "# we start from state 0 (sunny)\n",
    "pos[0] = 0\n",
    "# transition_times will store the time instants at which jumps/transitions happen\n",
    "transition_times = np.zeros(n_steps)\n",
    "# the random time to wait for the next transition\n",
    "# is drawn according to its distribution, as discussed in Remark 1\n",
    "# NOTE: in the formula for t_next we use w_star, the rate of the \"global\" Poisson clock\n",
    "t_next = -np.log(np.random.rand())/w_star\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    # the next state to visit will be extracted according to the probabilities\n",
    "    # stored in the row of P_bar corresponding to the current state.\n",
    "    # We extract a value pos[i] in (0,...,num_states-1) according to the discrete distribution P_bar[pos[i-1],:]\n",
    "    pos[i] = np.random.choice(nstates, p=P_bar[pos[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    # compute the waiting time to the next transition\n",
    "    t_next = -np.log(np.random.rand())/w_star\n",
    "\n",
    "# plot the trajectory for the first 20 jumps\n",
    "plt.plot(transition_times[0:20], pos[0:20], 'bo')\n",
    "plt.title('Trajectory for the first 20 jumps')\n",
    "\n",
    "# Estimate pi\n",
    "\n",
    "pi_estimate = np.zeros(nstates)\n",
    "# We have the time instants of all transitions, we now compute time intervals.\n",
    "# np.diff computes the n-th discrete difference of a vector.\n",
    "# Here we set n=1 to compute first difference, which is given by \n",
    "# intervals[i] = transition_times[i+1] - transition_times[i].\n",
    "# We also provide a value to append to transition_times prior to performing the difference\n",
    "# so that we can compute also the last interval: \n",
    "# transition_times[-1] + t_next is the end of the time horizon.\n",
    "intervals = np.diff(transition_times, n=1, append = transition_times[-1] + t_next)\n",
    "\n",
    "# for each state in the state space\n",
    "for state in range(nstates):\n",
    "    # identify the steps when we visited that state during the process\n",
    "    visits = np.argwhere(pos == state)\n",
    "    # the estimate of the invariant measure for that state is equal to the\n",
    "    # time spent on the state divided the total time of the process\n",
    "    pi_estimate[state] = np.sum(intervals[visits])/(transition_times[-1] + t_next)\n",
    "    \n",
    "print(\"Estimate of pi_bar:\", pi_estimate)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach: local clocks with rates w_i and matrix P\n",
    "\n",
    "# contruct the P matrix (instead of P_bar) and clock rates w\n",
    "w = np.sum(Lambda, axis=1)\n",
    "D = np.diag(w)\n",
    "P = np.linalg.inv(D) @ Lambda\n",
    "\n",
    "# set the number of steps in the simulation\n",
    "n_steps = 10000\n",
    "# pos will keep trace of the visited states\n",
    "pos = np.zeros(n_steps, dtype=int)\n",
    "# we start from state 0\n",
    "pos[0] = 0\n",
    "# transition_times will store the time instants at which\n",
    "# jumps/transitions happen\n",
    "transition_times = np.zeros(n_steps)\n",
    "# the random time to wait for the next transition\n",
    "# is drawn according to its distribution, discussed in Remark 2\n",
    "# NOTE: in the formula for t_next we use the rate of the clock of the current state, in this case w[0].\n",
    "t_next = -np.log(np.random.rand())/w[0]\n",
    "\n",
    "for i in range(1,n_steps):\n",
    "    # the next state to visit will be extracted according to the probabilities\n",
    "    # stored in the row of P corresponding to the current state.\n",
    "    pos[i] = np.random.choice(nstates, p=P[pos[i-1],:])\n",
    "    # store the time instant of the current transition\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    # compute the waiting time to the next transition\n",
    "    # NOTE: we use the rate w[pos[i]] of the clock of the current position\n",
    "    t_next = -np.log(np.random.rand())/w[pos[i]]\n",
    "\n",
    "# plot the trajectory for the first 20 jumps\n",
    "plt.plot(transition_times[0:20], pos[0:20], 'bo')\n",
    "plt.title('Trajectory for the first 20 jumps')\n",
    "\n",
    "# Estimate pi\n",
    "\n",
    "pi_estimate = np.zeros(nstates)\n",
    "\n",
    "intervals = np.diff(transition_times, n=1, append = transition_times[-1] + t_next)\n",
    "for node in range(nstates):\n",
    "    visits = np.argwhere(pos == node)\n",
    "    pi_estimate[node] = np.sum(intervals[visits])/(transition_times[-1] + t_next)\n",
    "print(\"Estimate of pi_bar:\", pi_estimate)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating with the third approach (link-based) is more complicated, because it requires to consider multiple Poisson clocks at the same time. Indeed, when $X(t)=i$, one should at each time-step:\n",
    "1. compute the random tick for each of the Poisson clocks of the links $(i,j)$ with rate $\\omega_{(i,j)} = \\Lambda_{ij}$\n",
    "2. select the link $(i,j)$ that ticked first (recall that each tick is random)\n",
    "3. Update the state of the Markov chain to $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. An ice-cream shops profit depends on the weather. Their profit per time unit is given by the following function\n",
    "\n",
    "$$\n",
    "f(X) = \\begin{cases} \n",
    "10 & \\text{if X = sunny}\\\\\n",
    "2 & \\text{if X = cloudy}\\\\\n",
    "1 & \\text{if X = rainy}\\\\\n",
    "0 & \\text{if X = snowy}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Simulate how the profit grows with time. Compute the average profit, both from the simulation and from the stationary distribution $\\bar{\\pi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of profit growth\n",
    "# I choose to simulate the CTMC following the first approach\n",
    "\n",
    "# set the number of steps in the simulation\n",
    "n_steps = 10000\n",
    "# payoff values corresponding to each state\n",
    "payoff = [10, 2, 1, 0]\n",
    "\n",
    "pos = np.zeros(n_steps, dtype=int)\n",
    "pos[0] = 0\n",
    "transition_times = np.zeros(n_steps)\n",
    "# since I'm following the first simulation approach, here I divide by the rate w_star of the global clock\n",
    "t_next = -np.log(np.random.rand())/w_star\n",
    "# we define a profit variable, which stores the comulative profit up to\n",
    "# the current time. For the first interval, which is t_next long, \n",
    "# the profit grows by payoff[pos[0]]*t_next\n",
    "profit = payoff[pos[0]]*t_next\n",
    "\n",
    "# we evolve the process as done before, increasing at each step the total profit\n",
    "for i in range(1,n_steps):\n",
    "    pos[i] = np.random.choice(nstates, p=P_bar[pos[i-1],:])\n",
    "    transition_times[i] = transition_times[i-1] + t_next\n",
    "    # compute the waiting time to the next transition\n",
    "    # NOTE: we use the rate w[pos[i]] of the clock of the current position\n",
    "    t_next = -np.log(np.random.rand())/w_star\n",
    "    # during the next interval, which is t_next long, the process will be in state\n",
    "    # pos[i] and the profit will grow by payoff[pos[i]]*t_next\n",
    "    profit = profit + payoff[pos[i]]*t_next\n",
    "    \n",
    "# the average profit is estimated as the overall profit obtained along the\n",
    "# simulation divided by the total time of the process\n",
    "average_profit = profit/(transition_times[-1] + t_next)\n",
    "print(\"Average profit\", average_profit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
