{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Flow optimization\n",
    "For this exercise, we will use CVXPY which is a Python-embedded modeling language for convex optimization problems. It allows to express problems in a natural way that follows the math, rather than in the restrictive standard form required by solvers (Matlab has the equivalent CVX).\n",
    "\n",
    "If CVXPY is not already in your environment, you can install it following the instructions reported [here](https://www.cvxpy.org/install/index.html). (for Anaconda in Windows run in your notebook the command **conda install -c conda-forge cvxpy**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separable convex network flow optimization arises in several applications, e.g., shortest paths between origin and destination, system optimum network flows, user optimum network flows in homogeneous routing games, electrical current flows in resistor networks.\n",
    "\n",
    "Given a multigraph $(V,E)$, an **exogenous network flow** is a vector ${\\nu} \\in \\mathrm{R}^V$ such that\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\sum_{i \\in V} \\nu_i = 0.\n",
    "\\end{equation}\n",
    "\n",
    "A network flow is a vector $f \\in \\mathrm{R}^E$ satisfying a positivity constraint and a mass conservation constraints, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    " f \\ge \\mathbf{0}, \\quad Bf = \\nu.\n",
    "\\end{equation}\n",
    "\n",
    "Every edge is endowed with a separable non-decreasing convex cost function $\\psi_e(f_e)$ such that $\\psi_e(0)=0$. \n",
    "\n",
    "Given an exogenous flow $\\nu$ and a network with node-edge matrix $B$, we study the following optimization problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\tf^* \\in \\ & \\underset{\\substack{f \\in \\mathrm{R}^{E}_+ \\\\ B f = \\nu}}{\\arg\\min}\n",
    "\t& & \\sum_{e \\in E} \\psi_e (f_e).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The ratio $\\psi_e(f_e) / f_e$ may be interpreted as the cost per unit of flow sent along the edge $e$. The convexity of $\\psi_e(f_e)$ is thus equivalent to requiring that the marginal cost for sending some flow on each edge is non-decreasing in the flow itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic applications with congestion\n",
    "\n",
    "## Pigou's example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pigou was the first to illustrate the role of externality in users' choices, and the difference between user optimum and system optimum flow distributions.\n",
    "\n",
    "We consider the traffic network with delay functions shown in figure below, which is assumed to\n",
    "have a unit exogenous inflow at node $o$ and a unit exogenous outflow at node $d$.\n",
    "\n",
    "![figure](pigou.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Determine the social (system) optimum flow distribution using CVXPY. The social optimum flow distribution is defined as the one minimizing the total delay $\\sum_{e\\in \\mathcal{E}} \\psi_e(f_e)$, where on every link $e \\in \\mathcal{E}$, the cost is $\\psi_e(f_e) = f_e \\tau_e(f_e)$ with $\\tau_e(f_e)$ being the delay function. (note that if $\\tau_e(f_e)$ is constant, i.e., if there is no congestion, the problem reduces to finding the shortest path)\n",
    "2. Determine the user optimum flow (Wardrop equilibrium) using CVXPY.\n",
    "3. Find tolls to align user optimum to social optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  **Compute social optimum flows.** The social optimum of the network flow problem can be computed with CVXPY using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Problem data.\n",
    "B = np.array([[1, 1],[-1,-1]])\n",
    "n_edges = B.shape[1]\n",
    "\n",
    "# exogenous flow vector: one unit of flow enters the origin and \n",
    "# exits the destination node\n",
    "nu = np.array([1, -1])\n",
    "# Q and l store the coefficients of the quadratic\n",
    "# and linear terms of the objective function.\n",
    "# In this case, the objective function is\n",
    "# f_1^2 + f_2\n",
    "Q = np.diag([1,0]) # diagonal matrix\n",
    "l = np.array([0,1]) # 1d array\n",
    "\n",
    "# Construct the problem\n",
    "f = cp.Variable(n_edges)\n",
    "objective = cp.Minimize(cp.quad_form(f,Q) + l.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "cost_opt = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "opt_flow = f.value\n",
    "print(\"Social optimal flow:\", opt_flow)\n",
    "print(\"Optimal cost:\", cost_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  **Compute Wardrop equilibrium.** \n",
    "\n",
    "**Definition**: The Wardrop equilibrium is a path flow distribution such that if a path $p$ is used, then the cost of the path is minimal, i.e.,\n",
    "\n",
    "$$\n",
    "z_p>0 \\implies c_p(z) \\le c_r(z), \\quad \\forall \\ r \\in \\mathcal{P},\n",
    "$$\n",
    "\n",
    "where $\\mathcal{P}$ is the path set, $f=Az$ is the projection of the path flow on the link set, and $c_p(z) = \\sum_{e: e \\in p} \\tau_e(f_e)$, namely the cost of a path is the sum of the delays that compose the path.\n",
    "\n",
    "We say that $f$ is a Wardrop equilibrium if it is induced by a Wardrop equilibrium $z$ via $f=Az$.\n",
    "\n",
    "**Theorem**: $f^{(0)}$ is a Wardrop equilibrium if and only if it is solution of a network flow optimization given that the cost functions $\\psi_e(f_e)$ are chosen as\n",
    "\n",
    "$$\n",
    "\\psi_e(f_e) = \\int_0^{f_e} \\tau_e(s)~\\mathrm{d}s.\n",
    "$$\n",
    "For our problem, the objective function is\n",
    "$$\n",
    "\\frac{f_1^2}{2}+f_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1, 1],[-1,-1]])\n",
    "n_edges = B.shape[1]\n",
    "\n",
    "# exogenous flow vector: one unit of flow enters the origin and \n",
    "# exits the destination node\n",
    "nu = np.array([1, -1])\n",
    "# Q and l store the coefficients of the quadratic\n",
    "# and linear terms of the objective function.\n",
    "Q = np.diag([1/2,0]) # diagonal matrix\n",
    "l = np.array([0,1]) # 1d array\n",
    "\n",
    "# Construct the problem.\n",
    "f = cp.Variable(n_edges)\n",
    "cp.quad_form(f,Q)\n",
    "objective = cp.Minimize(cp.quad_form(f,Q) + l.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result_w = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "print(\"Wardrop equilibrium:\", f.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the social cost under Wardrop equilibrium is not the solution of the optimization problem above. It is\n",
    "\n",
    "$$\n",
    "\\sum_{e}f_e^{(0)}\\tau_e(f_e^{(0)}),\n",
    "$$\n",
    "namely, namely the total travel time at the equilibrium. The Price of Anarchy is\n",
    "\n",
    "$$\n",
    "PoA = \\frac{\\sum_{e}f_e^{(0)}\\tau_e(f_e^{(0)})}{\\sum_{e}f_e^{*}\\tau_e(f_e^{*})}\n",
    "$$\n",
    "\n",
    "This observation immediately implies that the price of anarchy cannot be less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(f):\n",
    "    return f[0]*f[0]+f[1] # compute the social cost (total travel time) (f_1)^2+f_2\n",
    "\n",
    "cost_w = cost(f.value) # evaluate the social cost at the Wardrop equilibrium\n",
    "\n",
    "print(\"Wardrop cost:\", cost_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the price of anarchy, which is the ratio between the Wardrop cost and the optimal cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PoA = cost_w/cost_opt\n",
    "\n",
    "print(\"The price of anarchy:\", PoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** [Roughgarden et al., 2006] The maximum price of anarchy achievable with affine delay functions is $4/3 \\sim 1.33333...$.\n",
    "\n",
    "Thus, this simple example achieves the maximum possible price of anarchy!\n",
    "\n",
    "![figure](pigou.png)\n",
    "\n",
    "In other words, what is the price of anarchy? \n",
    "\n",
    "- Assume that the user choices are imposed by a central planner. The flow distribution emerging from this centralized decision scheme is the social optimum flow distribution.\n",
    "- In contrast with this, assume that users are free to choose their path based on other user choices. The flow distribution emerging from this uncoordinated selfish behaviour is the Wardrop equilibrium, in which every user selects a path with minimal delay.\n",
    "\n",
    "Note indeed that under system optimum flow distribution $(1/2, 1/2)$, the delay associated to each path (in this case, links) are\n",
    "\n",
    "$$\n",
    "\\tau_1(1/2) = 1/2, \\qquad \\tau_2(1/2) = 1,\n",
    "$$\n",
    "\n",
    "thus an agent that is using the edge 2, if free to choose, will switch to edge 1.\n",
    "\n",
    "There are many ways to improve the performance of the network. Two examples are:\n",
    "- to improve the infrastructure (e.g., improving the delay functions of the links, or adding new links), which is called **network design problem**;\n",
    "- to influence the agents' behaviour, in such a way to align user optimum flows with social optimum flows and reduce the inefficiency due to uncoordinated agent behaviour. This can be done in practice by information design, or adding tolls to the links of the network, which is the subject of the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to add tolls: marginal tolls\n",
    "The property of Wardrop equilibrium is that the cost of each used path (link, in this case) is the same.\n",
    "\n",
    "However, when switching from edge 2 to edge 1 (same argument may be applied to paths in more complicated networks), an infinitesimal agent produces a modification in the social cost which is\n",
    "\n",
    "$$\n",
    "\\psi_1'(f_1)-\\psi_2'(f_2) = \\tau_1(f_1)+f_1 \\tau_1'(f_1) - \\tau_2(f_2)-f_2 \\tau_2'(f_2),\n",
    "$$\n",
    "\n",
    "where $f_e \\tau_e'(f_e)$ is the derivative of the cost that the other users on edge $e$ will pay due to the addition of infinitesimal flow on edge $e$.\n",
    "\n",
    "The idea of marginal tolls is that, if a toll equal to $f_e^{*} \\tau_e'(f_e^{*})$ is added to each link $e$ (where $f^{*}$ denotes the social optimum flow distribution), then selfish users should pay not only for their delay, but also for the cost that make other users pay due to their choice.\n",
    "\n",
    "For a general $\\psi_e(f_e)$, $$\\omega_e^* = \\psi_e'(f_e^*) - \\tau_e(f_e^*).$$\n",
    "\n",
    "Thus, users minimize the cost that all the system pays for their choice, which makes the equilibrium coincide with the system optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compute Wardrop equilibrium with tolls.** Computing the Wardrop equilibrium as in 2., but with the link delays $\\tau_e(f_e)$ replaced by $\\tau^{(\\omega_e)}_e(f_e) = \\tau_e(f_e) + \\omega_e$, a CVXPY solution can be obtained by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1, 1],[-1,-1]])\n",
    "n_edges = B.shape[1]\n",
    "\n",
    "# exogenous flow vector: one unit of flow enters the origin and \n",
    "# exits the destination node\n",
    "nu = np.array([1, -1])\n",
    "\n",
    "# find marginal tolls: for each edge the toll equals f*_e tau_e'(f_e*)\n",
    "# for affine delays l_e + b_e f_e, this reads b_e f*_e\n",
    "# in this example b = [1,0]\n",
    "omega = opt_flow * np.array([1,0])\n",
    "\n",
    "# Construct the problem.\n",
    "f = cp.Variable(n_edges)\n",
    "objective = cp.Minimize(cp.quad_form(f, Q) + l.T @ f + omega.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "print(\"Wardrop equilibrium with tolls:\", f.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting flow vector is $f^{(\\omega)} = (1/2, 1/2)$, which is also the social optimum flow, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Braess' paradox\n",
    "\n",
    "Braess' paradox occurs when improving the network, e.g., adding a link, leads to a higher social cost under Wardrop equilibrium, or equivalently, removing a link reduces the social cost under Wardrop equilibrium .\n",
    "\n",
    "Let us see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from([0,1,2,3])\n",
    "\n",
    "G.add_edges_from([(0,1),(0,2),(1,2),(1,3),(2,3)])\n",
    "\n",
    "pos = {0:[0,0], 1:[2,1], 2:[2,-1], 3:[4,0]}\n",
    "\n",
    "labels = ['2x','x+1','x','x+1','2x']\n",
    "\n",
    "zip_operator = zip(G.edges(), labels)\n",
    "labels = dict(zip_operator)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color ='lightblue')\n",
    "\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, arrows=True, width=1)\n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels = labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge labels represent delay functions.\n",
    "\n",
    "**Social optimum flows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = -nx.linalg.graphmatrix.incidence_matrix(G, oriented=True).toarray()\n",
    "n_edges = len(G.edges)\n",
    "# exogenous flow vector: one unit of flow enters the origin and exits the destination node\n",
    "tau = 1\n",
    "nu = np.array([1, 0, 0, -1]) * tau\n",
    "\n",
    "Q = np.diag([2,1,1,1,2]) # diagonal matrix\n",
    "l = np.array([0,1,0,1,0]) # 1d array\n",
    "\n",
    "# Construct the problem.\n",
    "f = cp.Variable(n_edges)\n",
    "objective = cp.Minimize(cp.quad_form(f,Q) + l.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "cost_opt = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "print(\"Optimal flows:\", f.value)\n",
    "print(\"Optimal cost:\", cost_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wardrop equilibrium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = -nx.linalg.graphmatrix.incidence_matrix(G, oriented=True).toarray()\n",
    "n_edges = len(G.edges)\n",
    "# exogenous flow vector: one unit of flow enters the origin and \n",
    "# exits the destination node\n",
    "tau = 1\n",
    "nu = np.array([1, 0, 0, -1]) * tau\n",
    "# l stores the coefficients of the linear terms of the objective function, in this case the length of the links.\n",
    "# In this case, the objective function is\n",
    "\n",
    "Q = np.diag([1,1/2,1/2,1/2,1]) # diagonal matrix\n",
    "l = np.array([0,1,0,1,0]) # 1d array\n",
    "\n",
    "# Construct the problem.\n",
    "f = cp.Variable(n_edges)\n",
    "objective = cp.Minimize(cp.quad_form(f,Q) + l.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "print(\"Wardrop flows:\", f.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost, defined as \\sum_e f_e d_e(f_e)\n",
    "def cost(f):\n",
    "    return 2*f[0]*f[0]+f[1]*(f[1]+1)+f[2]*f[2]+f[3]*(f[3]+1)+2*f[4]*f[4]\n",
    "\n",
    "cost_w = cost(f.value) \n",
    "\n",
    "print(\"Wardrop cost:\", cost_w)\n",
    "\n",
    "PoA = cost_w/cost_opt\n",
    "\n",
    "print(\"The price of anarchy:\", PoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that under optimal flows the link $e_3$ is not used, while under Wardrop equilibrium $e_3$ is used, and $PoA>1$.\n",
    "\n",
    "This suggests that the inefficiency (since PoA>1) may be eliminated by removing the link $e_3$\n",
    "\n",
    "Let's try!\n",
    "\n",
    "The optimal flows are not affected by removing $e_3$. Let's compute the Wardrop equilibrium of the new network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G without $e_3$\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_nodes_from([0,1,2,3])\n",
    "\n",
    "G.add_edges_from([(0,1),(0,2),(1,3),(2,3)])\n",
    "\n",
    "pos = {0:[0,0], 1:[2,1], 2:[2,-1], 3:[4,0]}\n",
    "\n",
    "labels = ['2x','x+1','x+1','2x']\n",
    "\n",
    "zip_operator = zip(G.edges(), labels)\n",
    "labels = dict(zip_operator)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color ='lightblue')\n",
    "\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, arrows=True, width=1)\n",
    "\n",
    "nx.draw_networkx_edge_labels(G,pos,edge_labels = labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = -nx.linalg.graphmatrix.incidence_matrix(G, oriented=True).toarray()\n",
    "n_edges = len(G.edges)\n",
    "\n",
    "# exogenous flow vector: one unit of flow enters the origin and \n",
    "# exits the destination node\n",
    "tau = 1\n",
    "nu = np.array([1, 0, 0, -1]) * tau\n",
    "# l stores the coefficients of the linear terms of the objective function, in this case the length of the links.\n",
    "# In this case, the objective function is\n",
    "\n",
    "Q = np.diag([1,1/2,1/2,1]) # diagonal matrix\n",
    "l = np.array([0,1,1,0]) # 1d array\n",
    "\n",
    "# Construct the problem.\n",
    "f = cp.Variable(n_edges)\n",
    "objective = cp.Minimize(cp.quad_form(f,Q) + l.T @ f)\n",
    "constraints = [B @ f == nu, f >=0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# The optimal objective value is returned by `prob.solve()`.\n",
    "result = prob.solve()\n",
    "# The optimal value for f is stored in `f.value`.\n",
    "print(\"Wardrop flows:\", f.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost, defined as \\sum_e f_e d_e(f_e)\n",
    "def cost(f):\n",
    "    return 2*f[0]*f[0]+f[1]*(f[1]+1)+f[2]*(f[2]+1)+2*f[3]*f[3]\n",
    "\n",
    "cost_w = cost(f.value) \n",
    "\n",
    "print(\"Wardrop cost:\", cost_w)\n",
    "\n",
    "PoA = cost_w/cost_opt\n",
    "\n",
    "print(\"The price of anarchy:\", PoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wardrop cost decreases, which is the Braess' paradox.\n",
    "\n",
    "This is an example of network design problem in which links are removed instead of added or improved, exploiting Braess' paradox. Braess' paradox has been observed also in reality, not only in theoretical examples.\n",
    "\n",
    "**Remark**: the price of anarchy could be minimized also by adding marginal tolls, or by other toll vectors, e.g., by adding a large toll on link $e_3$ in such a way to prevent users to travel along that link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A sufficient conditions for PoA=1\n",
    "\n",
    "**Observation**: assume that delay functions are linear, i.e., $\\tau_e(f_e) = a_e f_e$. Then, \n",
    "\n",
    "$$\n",
    "f_e \\tau_e(f_e) = a_e f_e^2 = 2 \\int_{0}^{f_e} \\tau_e(s) ds,\n",
    "$$\n",
    "\n",
    "which implies that if the delay functions are linear, the equilibrium and the social optimum solves the same optimization problem, then user equilibria coincide with social optimum flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resistor networks\n",
    "Network flow optimization can be adapted to many applications, for instance electrical flows in resistor networks.\n",
    "\n",
    "In particular, social optimum traffic flows coincide with electrical current in a resistor network if:\n",
    "- the delay functions are linear, i.e., $\\tau_e(f_e) = a_e f_e$;\n",
    "- the non-negativity constraint $f_e \\ge 0$ is relaxed.\n",
    "\n",
    "The coefficients $a_e$ have to be interpreted as the resistance of the link $e$, and the throughput $\\tau$ as the electrical current injected in the network. It can also be proven that the Lagrangian multipliers associated to the mass conservation in the node set corresponds to electrical potential on the nodes of the resistor network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics\n",
    "In the first part of the lab we study linear averaging dynamics on graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $G=(V,E,W)$ be a weighted graph, and $x(t) \\in \\mathrm{R}^{V}$ denote the state of the nodes of the graph.\n",
    "\n",
    "The dynamics of $x(t)$ reads\n",
    "\n",
    "$$\n",
    "x(t+1) = Px(t),\n",
    "$$\n",
    "\n",
    "where $P$ is the normalized adjacency matrix.\n",
    "Among the applications, the most popular is opinion dynamics, where $x_i$ indicates the opinion of node $i$. This dynamics is known as French - De Groot.\n",
    "\n",
    "Note that we assume by convention that the opinion of node $i$ is influenced by the opinion of node $j$ if $P_{ij}>0$, i.e., the link $(i,j)$ has to be interpreted as $i$ watching $j$ and updating her opinion based on opinion of $j$.\n",
    "\n",
    "**Observation**: observe that $\\mathbf{1}$ is an equilibrium distribution, since $\\mathbf{1} = P \\mathbf{1}$ ($P$ is row-stochastic by construction), i.e., consensus distributions are equilibria of the dynamics.\n",
    "\n",
    "**Questions**: \n",
    "- what are the conditions under which the dynamics converges to an equilibrium?\n",
    "- what are the conditions under which all equilibria are consensus configurations?\n",
    "\n",
    "**Theorem**: assume that\n",
    "- its condensation graph has 1 sink;\n",
    "- the graph is aperiodic.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\lim_{t \\to +\\infty} x(t) = \\alpha \\mathbf{1},\n",
    "$$\n",
    "\n",
    "i.e., the agents get to consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why aperiodicity matters: example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,0)])\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "pos = {0:[0,0], 1:[0,2], 2:[2,2], 3:[2,0]}\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the graph is periodic because of every cycle has even length (the graph is bipartite and undirected, thus its period is 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign initial opinion to nodes and run the dynamics\n",
    "x = np.array([1,0,1,0])\n",
    "\n",
    "nodecolor=x\n",
    "\n",
    "# plot centrality at iteration 0\n",
    "plt.figure(1, figsize=(5,3))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=nodecolor,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds,\n",
    "         with_labels=True) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes with opinion 1 are red, nodes with opinion 0 are white.\n",
    "\n",
    "After one iteration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = P @ x\n",
    "\n",
    "nodecolor=x\n",
    "\n",
    "plt.figure(1, figsize=(5,3))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=nodecolor,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds,\n",
    "         with_labels=True) \n",
    "\n",
    "print(\"x(1):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 iterations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = P @ P @ P @ P @ x\n",
    "\n",
    "nodecolor=x\n",
    "\n",
    "plt.figure(1, figsize=(5,3))\n",
    "# we draw the graph with same node position \"pos\" defined above\n",
    "nx.draw(G,pos,\n",
    "         nodelist=list(G.nodes()), \n",
    "         # node's color reflects centrality values (higher dc = darker color)\n",
    "         node_color=nodecolor,\n",
    "         font_size=8,\n",
    "         # node's colors are on the red scale\n",
    "         cmap=plt.cm.Reds,\n",
    "         with_labels=True) \n",
    "\n",
    "print(\"x(5):\", x)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodicity of the graph does not allow a proper mixing of the opinions!\n",
    "\n",
    "Let us add a link to make the graph aperiodic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,0)])\n",
    "G.add_edge(0,2)\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run again the dynamics with same initial conditions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,0,1,0])\n",
    "\n",
    "x = P @ x\n",
    "print(\"x(1):\", x)\n",
    "\n",
    "x = P @ x\n",
    "print(\"x(2):\", x)\n",
    "\n",
    "x = P @ x\n",
    "print(\"x(3):\", x)\n",
    "\n",
    "x = P @ x\n",
    "print(\"x(4):\", x)\n",
    "\n",
    "x = P @ x\n",
    "print(\"x(5):\", x)\n",
    "\n",
    "x = P @ P @ P @ P @ P @ x\n",
    "print(\"x(10):\", x)\n",
    "\n",
    "x = P @ P @ P @ P @ P @ x\n",
    "print(\"x(15):\", x)\n",
    "\n",
    "x = P @ P @ P @ P @ P @ x\n",
    "print(\"x(20):\", x)\n",
    "\n",
    "x = P @ P @ P @ P @ P @ x\n",
    "print(\"x(25):\", x)\n",
    "\n",
    "x = P @ P @ P @ P @ P @ x\n",
    "print(\"x(30):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics reachs an equilibrium, which is a consensus.\n",
    "\n",
    "### Two questions\n",
    "- How fast the consensus is achieved?\n",
    "- What is the consensus value?\n",
    "\n",
    "We will answer to these questions later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 1 sink in the condensation graph: example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(1,2),(2,1),(1,0),(0,2),(3,2),(3,4),(4,5),(5,4),(6,4),(5,6)])\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many sink components does this graph have?\n",
    "\n",
    "Let us compute the condensation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG = nx.algorithms.components.condensation(G)\n",
    "\n",
    "nx.draw(CG)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condensation graph has 2 sinks. Thus, the sufficient conditions under which the dynamics converges to consensus does not hold.\n",
    "\n",
    "Let us figure out why by an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# define initial condition\n",
    "x = [1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "for n in range(100):\n",
    "    x = P @ x\n",
    "print(\"x(100):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamics does not reach consensus, because there are two communities of agents that do not communicate each other, and achieve consensus separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add a link in such a way that the condensation graph has now a single sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(6,3)\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG = nx.algorithms.components.condensation(G)\n",
    "nx.draw(CG)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# define initial condition\n",
    "x = [1, 1, 0, 1, 1, 0, 0]\n",
    "\n",
    "for n in range(99):\n",
    "    x = P @ x\n",
    "print(\"x(100):\", x, \"\\n\")\n",
    "\n",
    "for n in range(199):\n",
    "    x = P @ x\n",
    "print(\"x(200):\", x, \"\\n\")\n",
    "\n",
    "for n in range(999):\n",
    "    x = P @ x\n",
    "print(\"x(1000):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reach consensus!!\n",
    "\n",
    "## Consensus value\n",
    "\n",
    "How is the consensus value computed?\n",
    "\n",
    "Let $\\pi$ denote the normalized left dominant eigenvector of $P$, i.e., the normalized $\\pi$ such that $P' \\pi = \\pi$ (which is unique because the graph has one sink only).\n",
    "We use the fact that\n",
    "\n",
    "$$\n",
    "\\pi' x(t) = \\pi' P x(t-1) = \\pi' x(t-1),\n",
    "$$\n",
    "\n",
    "thus $\\pi' x(t)$ is constant along the dynamics. Thus,\n",
    "\n",
    "$$\n",
    "\\pi' x(0) = \\pi' \\lim_{t \\to + \\infty} x(t) = \\alpha \\pi' \\mathbf{1} = \\alpha.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie computation above says that the consensus value $\\alpha$ is the weighted average of the initial conditions of the nodes, where the weights are given by the (unique) invariant distribution $\\pi$.\n",
    "\n",
    "Recall that $\\pi$ solving $\\pi = P' \\pi$ is also the invariant distribution centrality, thus the more a node is central the more its initial opinion affects the consensus value.\n",
    "\n",
    "Let us explore the form of $\\pi$ for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,v = np.linalg.eig(P.T)\n",
    "\n",
    "# selects the eigenvalue 1 and print the eigenvector\n",
    "for index in [i for i in range(len(G)) if np.isclose(w[i],1)]: \n",
    "    pi = v[:,index].real  # -> eigenvectors are complex but pi is real, so we convert it to real\n",
    "    pi = pi/np.sum(pi)\n",
    "    print(\"pi\", index, \"=\", pi)\n",
    "    \n",
    "nx.draw(G,pos,with_labels=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The invariant distribution centrality is 0 for all the nodes that do not belong to the sink of the condensantion graph!\n",
    "\n",
    "This implies that the initial opinion of the nodes not belonging to the sink are negligible for the consensus value.\n",
    "\n",
    "The intuition for this is that the nodes 0, 1 and 2 do not care of the other nodes, because are not out-connected to any other node out of the component, and they reach consensus because the induced subgraph on 0,1,2 is aperiodic and strongly connected.\n",
    "Thus, their evolution is not affected by other nodes. Conversely, the other nodes update their opinion based on 2 also, thus eventually they tend to agree to the opinion of node 2 (and thus 0 and 1 as well).\n",
    "\n",
    "Let us modify the initial condition of nodes 3,4,5,6 to observe that the consensus value is not affected by this modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial condition\n",
    "x = [1, 1, 0.5, 50, 50, 8, 12]\n",
    "\n",
    "for n in range(99):\n",
    "    x = P @ x\n",
    "print(\"x(100):\", x, \"\\n\")\n",
    "\n",
    "for n in range(200):\n",
    "    x = P @ x\n",
    "print(\"x(300):\", x, \"\\n\")\n",
    "\n",
    "for n in range(1000):\n",
    "    x = P @ x\n",
    "print(\"x(1300):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed of convergence\n",
    "\n",
    "Let us work now for undirected (the following argument holds only for directed) connected graphs. If the graph is also aperiodic, the dynamics is guaranteed to converge to consensus.\n",
    "\n",
    "Let $\\lambda:=\\max \\{\\lambda_2,|\\lambda_n|\\}$, where $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ are the eigenvalues of $P$. Recall that $\\lambda_n \\ge -1$ by construction of $P$. \n",
    "\n",
    "It is known that the dynamics reaches consensus exponentially fast. In particular, the distance from consensus at time $t$ is in some sense proportional to $\\lambda^t$ (as shown in the lecture notes).\n",
    "\n",
    "Thus, if $\\lambda$ is close to $1$, then the convergence is slow, whereas if $\\lambda$ is small the convergence is faster.\n",
    "\n",
    "Note also that for periodic graphs, by Perron-Frobenius theorem have $\\lambda_n = -1$ (thus $\\lambda=1$), then the convergence to consensus is not achieved. This is consistent with the theory of consensus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: note that every periodic strongly connected graph can be made periodic by adding at least a selfloop in the graph. Indeed, if a selfloop $(i,i)$ is added, then the period of node $i$ is 1. Since all the nodes in the same connected component have same period, then all the nodes have period $1$ and the graph is aperiodic.\n",
    "\n",
    "To avoid periodic graphs, sometimes it is useful to introduce the **lazy dynamics** obtained by replacing $P$ with \n",
    "\n",
    "$$\n",
    "\\frac{P+\\mathbf{I}}{2}\n",
    "$$\n",
    "\n",
    "This is equivalent to adding selfloops to each node in the graph with weight equivalent to the degree of the node itself.\n",
    "\n",
    "An interpretation for the lazy dynamics is that nodes have some inertia in the opinion. Instead of averaging over the opinions of the neighbors, they also take into account their opinion at the previous step. In fact,\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\frac{x_i(t) + \\sum_{j} P_{ij} x_j(t)}{2}.\n",
    "$$\n",
    "\n",
    "Let us go back to the initial periodic example, and show that the lazy dynamics converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1),(1,2),(2,3),(3,0)])\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pl = P/2 + np.diag(np.ones(4))/2\n",
    "\n",
    "x = np.array([1, 0, 1, 0])\n",
    "\n",
    "for n in range(9):\n",
    "    x = Pl @ x\n",
    "print(\"x(10):\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fast is the convergence of lazy dynamics?\n",
    "\n",
    "Note that $\\lambda_n(P_{lazy}) = 1/2 + \\lambda_n(P)/2 \\ge 1/2 + (-1/2) = 0$.\n",
    "\n",
    "Thus, $\\lambda = \\lambda_2$. \n",
    "\n",
    "In the lazy dynamics, the speed convergence is governed by $\\lambda_2$. Let us now define the relaxation time as\n",
    "\n",
    "$$\n",
    "\\tau_{rel} = \\frac{1}{1-\\lambda_2}\n",
    "$$\n",
    "\n",
    "For undirected graphs, $\\lambda_2$ is related to the level of connectedness of the graph. In particular, if the graph is well connected, $\\lambda_2$ is smaller and the convergence to consensus is faster.\n",
    "\n",
    "We do not investigate the details on how to define \"connectedness\" of graphs in proper way here. However, we can verify by a simple example how connectedness of the graph influences the speed of convergence.\n",
    "\n",
    "Let us consider a **cycle graph** with 1000 nodes.\n",
    "\n",
    "For the cycle graph one can show (by the theory of circulant matrices) that\n",
    "\n",
    "$$\n",
    "\\lambda_2(P) = \\cos \\left(\\frac{2\\pi}{n}\\right).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to + \\infty} \\lambda(P_{lazy}) = \\frac{1}{2} + \\lim_{n \\to + \\infty} \\frac{1}{2} \\cos \\frac{2\\pi}{n} = 1-\\frac{\\pi^2}{n^2},\n",
    "$$\n",
    "\n",
    "and the relaxation time for the cycle is\n",
    "\n",
    "$$\n",
    "\\tau_{rel} = \\frac{n^2}{\\pi^2}.\n",
    "$$\n",
    "\n",
    "This means that the convergence is achieved exponentially with a rate that scales quadratically with $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "G = nx.cycle_graph(n)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(n))/2\n",
    "\n",
    "# let us start with random initial conditions\n",
    "x = np.random.rand(n)\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the complete graph, which is the most connected graph by definition. For the complete graph,\n",
    "\n",
    "$$\n",
    "W=\\mathbf{1}\\mathbf{1}'-I.\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{1}\\mathbf{1}'$ has equal columns, it has rank $1$. Thus $\\mathbf{1}\\mathbf{1}'$ has $n-1$ eigenvalues equal to $0$. The remaining eigenvalue can be found by using the fact that the sum of the eigenvalues is the trace of the matrix. Since $\\text{trace}(\\mathbf{1}\\mathbf{1}'-I)=0$, the spectrum of $W$ is\n",
    "\n",
    "$$\n",
    "\\sigma_W = \\{n-1,-1,\\cdots,-1\\}\n",
    "$$\n",
    "\n",
    "Since the complete graph is regular and every node has degree $n-1$, $P=W/(n-1)$\n",
    "Thus, the spectrum of $P$ is\n",
    "\n",
    "$$\n",
    "\\sigma_P = \\{1,-\\frac{1}{n-1},\\cdots,-\\frac{1}{n-1}\\},\n",
    "$$\n",
    "\n",
    "and $P_{lazy}$ has spectrum\n",
    "\n",
    "$$\n",
    "\\sigma_{P_{lazy}} = \\{1,-\\frac{1}{2(n-1)}+\\frac{1}{2},\\cdots,-\\frac{1}{2(n-1)}+\\frac{1}{2}\\}\n",
    "$$\n",
    "\n",
    "This implies that $\\lambda_2 = \\frac{1}{2}-\\frac{1}{2(n-1)} = \\frac{n-2}{2(n-1)}$ and the relaxation time for large $n$ in the complete graph tends to\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to +\\infty} \\tau_{rel} = 2,\n",
    "$$\n",
    "\n",
    "i.e., even for infinite $n$ the relaxation time is finite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(n)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(n))/2\n",
    "\n",
    "# let us start with random initial conditions\n",
    "x = np.random.rand(n)\n",
    "\n",
    "variance = np.var(x)\n",
    "n=0\n",
    "\n",
    "while (variance>0.001):\n",
    "    x = Pl @ x\n",
    "    n=n+1\n",
    "    variance = np.var(x)\n",
    "\n",
    "print('Number of iteration for convergence:', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How fast is the convergence of (not lazy) averaging dynamics?\n",
    "\n",
    "In lazy dynamics the speed of convergence is described by $\\lambda_2$. In standard averaging dynamics, also $\\lambda_n$ plays a role. We recall indeed that the speed of convergence is described by\n",
    "\n",
    "$$\n",
    "\\lambda := \\max\\{\\lambda_2,|\\lambda_n|\\},\n",
    "$$\n",
    "\n",
    "where $\\lambda_n$ is the smallest eigenvalue of $P$.\n",
    "\n",
    "While $\\lambda_2$ is related to the level of connectedness of the graph, we shall see in a qualititive manner the role of $\\lambda_n$ by examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us construct a regular graph with 6m nodes, and degree of each node equal to m\n",
    "m = 5\n",
    "G = nx.cycle_graph(6*m)\n",
    "\n",
    "n_nodes = 6*m\n",
    "\n",
    "for n in range(6*m):\n",
    "    for i in range(m):\n",
    "        G.add_edge(n,(6*(i+1)+n-3) % (6*m))\n",
    "\n",
    "pos = nx.circular_layout(G)\n",
    "\n",
    "nx.draw(G,pos,with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,v = np.linalg.eig(P)\n",
    "w = w.real\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is bipartite, hence $-1$ is in the spectrum and the convergence to consensus is not guaranteed.\n",
    "\n",
    "Let us now add a single edge to break the periodicity while mantaining the total number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(0,2)\n",
    "G.remove_edge(0,9)\n",
    "\n",
    "nx.draw(G,pos,with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "w,v = np.linalg.eig(P)\n",
    "w = w.real\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is not periodic now. However, $|\\lambda_n| \\sim 1$, which implies that the convergence of the dynamics is slow. On the other hand, $\\lambda_2$ is \"small\" ($\\sim 1/2$), thus the lazy dynamics converges quickly.\n",
    "\n",
    "In some sense, $|\\lambda_n| \\sim 1$ reflects the fact that the graph is almost periodic. Let us see this with an example, by comparing the speed in convergence of the lazy dynamics and the standard dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a larger graph\n",
    "m = 50\n",
    "G = nx.cycle_graph(6*m)\n",
    "\n",
    "G.add_edge(0,2)\n",
    "\n",
    "n_nodes = 6*m\n",
    "\n",
    "for n in range(6*m):\n",
    "    for i in range(m):\n",
    "        G.add_edge(n,(6*(i+1)+n-3) % n_nodes)\n",
    "        \n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# let us start with random initial condition and standard dynamics\n",
    "x0 = np.random.rand(G.number_of_nodes())>1/2\n",
    "x = x0\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.00001):\n",
    "    x = P @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "    if t>100000:\n",
    "        t='inf'\n",
    "        break\n",
    "        \n",
    "print('Number of iterations for convergence of standard dynamics:', t)\n",
    "\n",
    "# same initial condition, lazy dynamics\n",
    "x = x0\n",
    "\n",
    "# Construct lazy P\n",
    "Pl = P/2 + np.diag(np.ones(G.number_of_nodes()))/2\n",
    "\n",
    "variance = np.var(x)\n",
    "t=0\n",
    "\n",
    "while (variance>0.00001):\n",
    "    x = Pl @ x\n",
    "    t=t+1\n",
    "    variance = np.var(x)\n",
    "    if t>100000:\n",
    "        t='inf'\n",
    "        break\n",
    "        \n",
    "print('Number of iterations for convergence of lazy dynamics:', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisdom of crowds\n",
    "Consider a graph, and assume that state of each node represents a noisy estimate of the real state $\\mu$, i.e.,\n",
    "\n",
    "$$\n",
    "x_i = \\mu + y_i,\n",
    "$$\n",
    "\n",
    "with $E[y_i]=0$, and the variance $\\sigma^2 (y_i) = \\sigma^2$ for each $i$.\n",
    "\n",
    "Assume that the graph is connected and aperiodic Eventually, the agents will reach consensus, i.e., $\\lim_{t \\to +\\infty} x(t) = \\alpha \\mathbf{1}$. \n",
    "\n",
    "**Question**: what is the consensus value $\\alpha$?\n",
    "\n",
    "Notice that $\\alpha = \\pi' (\\mu \\mathbf{1} + y) = \\mu + \\pi'y$, then\n",
    "\n",
    "$$\n",
    "E[\\alpha] = \\mu + \\pi' E[y] = \\mu\n",
    "$$\n",
    "\n",
    "so the final estimate of the network is unbiased. Moreover,\n",
    "\n",
    "$$\n",
    "\\quad \\sigma_{\\alpha}^2 = \\sigma^2 \\sum_{i} \\pi_i^2 < \\sigma^2,\n",
    "$$\n",
    "\n",
    "because $\\sum_{i} \\pi_i^2 <1$ unless the graph has a unique sink node. The interesting observation is that the estimate $\\alpha$ has a smaller variance than $\\sigma$, i.e., the crowd is able to reconstruct a more precise estimate of the real state than the single agents of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify this on a complete graph, where $\\pi_i = 1/n$, thus $\\sigma_\\alpha^2 = \\sigma^2/n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(100)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "# start with random initial states and run the dynamics 200 times\n",
    "# store in alfa_err the consensus values at each run\n",
    "alfa_err = np.zeros(200)\n",
    "\n",
    "for i in range(200):\n",
    "# rand returns uniformly random values in [0,1], thus mu = 1/2\n",
    "    x = np.random.rand(100)\n",
    "    for n in range(500):\n",
    "        x = P @ x\n",
    "    alfa_err[i] = (1/2 - np.mean(x))*(1/2 - np.mean(x))\n",
    "\n",
    "print(\"Variance of the node states:\", 1/12)\n",
    "print(\"Variance of the consensus state:\", np.mean(alfa_err), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the variance of $\\alpha$ is about $1/100$ of the original variance.\n",
    "\n",
    "Note that $\\sum_{i} \\pi_i^2$ tends to 1 if one node has almost all the centrality, and is minimal when the nodes have the same centrality (as in the complete graph, or in the cycle graph). This implies that if a graph is more democratic, then the consensus algorithm leads to better estimates of the true state. If a few nodes have all the centralities, the consensus value is less reliable.\n",
    "\n",
    "Let us see this with a another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(10)\n",
    "G = nx.Graph.to_directed(G)\n",
    "G.remove_edges_from([(0,1),(0,9)])\n",
    "G.add_edge(0,0)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with random initial states and run the dynamics\n",
    "alfa_err = np.zeros(200)\n",
    "\n",
    "for i in range(200):\n",
    "# rand returns random values in [0,1], thus \\mu = 1/2\n",
    "    x = np.random.rand(10)\n",
    "    var = np.var(x)\n",
    "    for n in range(500):\n",
    "        x = P @ x\n",
    "    alfa_err[i] = (1/2 - np.mean(x))*(1/2 - np.mean(x))\n",
    "\n",
    "print(\"Expected variance of the node states:\", 1/12)\n",
    "print(\"Empirical variance of the consensus state:\", np.mean(alfa_err), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph the consensus value is exactly the initial state of node $0$, because the condensation graph has 1 sink only, which is node $0$.\n",
    "\n",
    "$$\n",
    "\\pi = \\delta^{(0)}.\n",
    "$$\n",
    "\n",
    "This graph is the opposite of the complete graph, in the sense that the invariant distribution centrality is totally on node $0$. Thus the variance of the consensus state equals the variance of the single node.\n",
    "\n",
    "Note that this argument holds independently of the size of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: distributed computation of average\n",
    "\n",
    "Let the node set describe a set of sensors that are deployed in some regions in order to collect measurements of some quantity of interest (for example, the temperature). \n",
    "\n",
    "Assume that these sensors have limited communication and computation capabilities that allow each of them to exchange information only with those other sensors that are close enough in space. \n",
    "\n",
    "Let the graph $G = (V, E)$ describe the pattern of closeness among the sensors $i$ and $j$ so that there is an undirected link between node $i$ and node $j$ if they can communicate to each other (possibly using link weights decreasing with distance). Then, one can design a distributed algorithm for computing the average of the sensor's measurements based on the averaging dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i(0)$ be the measurement of each node. \n",
    "\n",
    "We are interested in designing an iterative distributed algorithm that allows the nodes to compute\n",
    "\n",
    "$$\n",
    "x = \\frac{1}{n}\\sum_i x_i(0)\n",
    "$$\n",
    "\n",
    "**First attempt**: we run a consensus algorithm. Since the graph is undirected, $\\pi_i = \\frac{w_i}{\\sum_j w_j}$. Thus, the algorithm converges to a consensus $\\alpha \\mathbf{1}$ such that\n",
    "\n",
    "$$\n",
    "\\alpha = \\sum_i \\pi_i x_i(0) = \\sum_{i} \\frac{w_i}{w} x_i(0).\n",
    "$$\n",
    "\n",
    "If each edge knows its degree $w_i$, each node can rescale its initial state, i.e., $y_i(0) = \\frac{x_i(0)}{w_i}$. The consensus algorithm for the variable $y_i$ thus converges to\n",
    "\n",
    "$$\n",
    "\\alpha_y = \\sum_{i} \\frac{w_i}{w} y_i(0) = \\frac{1}{w} \\sum_{i} x_i(0).\n",
    "$$\n",
    "\n",
    "If we assume that each node knows the average degree of the network, thus\n",
    "\n",
    "$$\n",
    "x = \\alpha_y \\frac{w}{n} = \\alpha_y \\overline{w}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Fix node positions on all pictures according to spring layout\n",
    "pos = nx.spring_layout(G) \n",
    "nx.draw_networkx(G, pos)\n",
    "\n",
    "n_nodes = len(G)\n",
    "\n",
    "x = np.random.rand(n_nodes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us run the consensus algorithm for y\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "y = x/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    y = P @ y\n",
    "\n",
    "print(\"Average state:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus on y\n",
    "print(\"Average computed distributively\", y[0] * np.sum(degrees) / n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm works! Unfortunately, requiring that each node knows the average degree of the network is not realistic (and not distributed).\n",
    "\n",
    "However, there exists another way to solve the problem.\n",
    "\n",
    "**Second attempt**: we run a second averaging dynamics, with initial condition $z_i(0) = \\frac{1}{w_i}$.\n",
    "\n",
    "This converges to\n",
    "\n",
    "$$\n",
    "\\alpha_z = \\lim_{t \\to + \\infty} z_i(t) = \\sum_i z_i(0) \\frac{w_i}{w} = \\sum_{i} \\frac{1}{w} = \\frac{1}{\\overline{w}}\n",
    "$$\n",
    "\n",
    "By combining the two, each node can estimate the average estimate by \n",
    "\n",
    "$$\n",
    "\\frac{\\lim_{t \\to + \\infty} y_i(t)}{\\lim_{t \\to + \\infty} z_i(t)} = \\frac{\\alpha_y}{\\alpha_z} = \\frac{x}{\\overline{w}} \\cdot \\overline{w} = x.\n",
    "$$\n",
    "\n",
    "This method does not require any global knowledge on the network and it is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us implement this\n",
    "\n",
    "z = 1/degrees\n",
    "\n",
    "for t in range(1000):\n",
    "    z = P @ z\n",
    "\n",
    "print(\"average state:\", np.mean(x))\n",
    "# choose arbitrarly the first node, but all the nodes reach consensus both on y and z\n",
    "print(\"average computed distributively\", y[0] / z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear averaging dynamics with stubborn agents\n",
    "We study how to simulate the linear averaging dynamics on graphs in presence of stubborn agents.\n",
    "\n",
    "We focus on the optimal placement problem, which consists of optimally chosing the position of a stubborn node on the graph in order to maximize its influence on the asymptotic outcome of the dynamics.\n",
    "\n",
    "Let us first summarize the theory in presence of stubborn agents.\n",
    "\n",
    "We are given a network, where the agents $V$ are divided in regular agents $R$ and stubborn agents $S$. The regular agents update their opinion $x(t)$ according to the standard DeGroot model, while the stubborn agents do not update their opinions, so that $u(t) \\equiv u$.\n",
    "\n",
    "Let $Q=P|_{R \\times R}$ and $E=P|_{R \\times S}$.\n",
    "\n",
    "Thus, the dynamics for the regular agents read\n",
    "\n",
    "$$\n",
    "x(t+1) = Qx(t) + Eu.\n",
    "$$\n",
    "\n",
    "Under some assumptions (see the lecture notes for details) the dynamics converges to \n",
    "\n",
    "$$\n",
    "x^* = (\\mathbf{I}-Q)^{-1}Eu.\n",
    "$$\n",
    "\n",
    "Note that, in contrast with the dynamics without stubborn nodes:\n",
    "- the asymptotic state is not a consensus;\n",
    "- the asymptotic state does not depend on the initial opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We start by implementing the averaging dynamics with stubborn nodes. \n",
    "\n",
    "To illustrate this procedure, we will analyse the following example that involves a $3 \\times 4$ grid graph $\\mathcal G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.generators.lattice.grid_graph(dim=[3,4])\n",
    "n_nodes = len(G)\n",
    "print(\"Number of nodes:\", n_nodes)\n",
    "\n",
    "# labels of nodes are couples: (column,row)\n",
    "nx.draw_spectral(G, with_labels=True)\n",
    "\n",
    "print(G.nodes())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dictionary that maps the label of nodes  \n",
    "# (from (0,0) to (2,1)) to their index (from 0 to n_nodes-1)\n",
    "indices = dict()\n",
    "for i in range(n_nodes):\n",
    "    indices[list(G.nodes)[i]] = i\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "    \n",
    "# Stubborn and regular nodes\n",
    "stubborn = [(0,0), (3,2)];\n",
    "stubborn_id = [indices.get(key) for key in stubborn]\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to stubborn nodes\n",
    "u = [0,1]\n",
    "\n",
    "# P matrix\n",
    "A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "A = A.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(A,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ A\n",
    "\n",
    "# Submatrices\n",
    "# Using ix_ one can construct index arrays that will \n",
    "# index a cross product. \n",
    "# a[np.ix_([1,3],[2,5])] returns the array [[a[1,2] a[1,5]], [a[3,2] a[3,5]]].\n",
    "Q = P[np.ix_(regular_id, regular_id)]\n",
    "E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "# Sample a random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = np.average(x_final)\n",
    "print(\"Average asymptotic opinion:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the dynamics does not converge to consensus. Moreover, in contrast with averaging without input stubborn nodes, we can verify that the asymptotic equilibrium does not depend on the initial condition. Furthermore, the dynamics converge to an equilibrium even though the graph is periodic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample another random initial condition for regular nodes\n",
    "ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn_id,0] = u;\n",
    "x[regular_id,0] = ic;\n",
    "print(\"Initial state:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "    x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "print(\"Final state:\")\n",
    "x_final = x[:,n_iter-1]\n",
    "for key in indices.keys():\n",
    "    print(key, x_final[indices[key]])\n",
    "    \n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(n_nodes):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal placement of stubborn nodes\n",
    "Suppose that node $(0,0)$ is stubborn with opinion $u_{(0,0)}=0$. We want to find the optimal position $(i,j)$ of a stubborn node with opinion $1$ in order to maximize the asymptotic average opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple approach is to consider all possible positions $(i,j)$ and pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "n_iter = 50;\n",
    "\n",
    "# We will store final opinion vectors and \n",
    "# average of final opinions in dictionaries\n",
    "# where the key is the position (i,j) of the \n",
    "# 1-stubborn agent\n",
    "final_opinions = dict()\n",
    "average_opinion = dict() \n",
    "\n",
    "\n",
    "for (i,j) in G.nodes:\n",
    "    # Position (0,0) is occupied by the 0-stubborn node\n",
    "    if (i,j)==(0,0):\n",
    "        continue\n",
    "        \n",
    "    # Stubborn and regular nodes\n",
    "    stubborn = [(0,0), (i,j)];\n",
    "    stubborn_id = [indices.get(key) for key in stubborn]\n",
    "    regular = [node for node in G.nodes if node not in stubborn]\n",
    "    regular_id = [id for id in range(n_nodes) if id not in stubborn_id]\n",
    "    print(\"Stubborn nodes:\", stubborn)\n",
    "\n",
    "    # Input to stubborn nodes\n",
    "    u = [0,1]\n",
    "\n",
    "\n",
    "    # P matrix\n",
    "    A = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "    A = A.toarray() # convert A to a numpy array\n",
    "    degrees = np.sum(A,axis=1)\n",
    "    D = np.diag(degrees)\n",
    "    P = np.linalg.inv(D) @ A\n",
    "\n",
    "    # Submatrices\n",
    "    Q = P[np.ix_(regular_id, regular_id)]\n",
    "    E = P[np.ix_(regular_id, stubborn_id)]\n",
    "\n",
    "    # Sample a random initial condition for regular nodes\n",
    "    ic = np.random.uniform(0,1,len(regular))\n",
    "\n",
    "    # Set the initial condition for the dynamics\n",
    "    x = np.zeros((n_nodes,n_iter))\n",
    "    x[stubborn_id,0] = u;\n",
    "    x[regular_id,0] = ic;\n",
    "\n",
    "    for t in range(1,n_iter):\n",
    "        x[regular_id, t] = Q @ x[regular_id, t-1] + E @ x[stubborn_id, t-1]\n",
    "        x[stubborn_id, t] = x[stubborn_id, t-1];\n",
    "\n",
    "    final_opinions[(i,j)] = x[:,n_iter-1]\n",
    "    average_opinion[(i,j)] = np.average(final_opinions[(i,j)])\n",
    "    print(\"Average opinion:\", average_opinion[(i,j)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the dependence of the average asymptotic opinion on the position of the $1$-stubborn node we can plot the grid graph by setting each node's size and color according to the magnitude of the average asymptotic opinion when the $1$-stubborn is placed in such node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a dummy (0,0) entry to the dictionary\n",
    "# to make its size = n_nodes\n",
    "average_opinion[(0,0)] = 0\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(10*average_opinion[node]) for node in G.nodes],\n",
    "        node_color= [average_opinion[node] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal placements of the 1-stubborn player are the maximizers of the final average opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the average opinion values from dict_values to numpy array\n",
    "avg = np.fromiter(average_opinion.values(),dtype=float)\n",
    "\n",
    "optimal_place = [place for place in average_opinion.keys() if average_opinion[place]==np.max(avg)]\n",
    "print(\"Optimal placements:\", optimal_place, \"\\n\")\n",
    "\n",
    "# print the final opinions under optimal placement\n",
    "opt_final = final_opinions.get(*optimal_place)\n",
    "print(\"Final opinions after optimal placement:\", opt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the asymptotic opinions of the nodes when the stubborn is placed in (1,1)\n",
    "\n",
    "plt.figure(1, figsize=(7,3))\n",
    "nx.draw(G, \n",
    "        pos = nx.spectral_layout(G),\n",
    "        with_labels=True, \n",
    "        node_size = [np.exp(8*opt_final[indices.get(node)]) for node in G.nodes],\n",
    "        node_color= [opt_final[indices.get(node)] for node in G.nodes],\n",
    "        font_size=8,\n",
    "        # node's colors are on the red scale\n",
    "        cmap=plt.cm.Reds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Back to an old example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.cycle_graph(10)\n",
    "G = nx.Graph.to_directed(G)\n",
    "G.remove_edges_from([(0,1),(0,9)])\n",
    "G.add_edge(0,0)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "n_nodes = G.number_of_nodes()\n",
    "\n",
    "# Construct P\n",
    "W = nx.adjacency_matrix(G) # -> return type is scipy.sparse.csr_matrix\n",
    "W = W.toarray() # convert A to a numpy array\n",
    "degrees = np.sum(W,axis=1)\n",
    "D = np.diag(degrees)\n",
    "P = np.linalg.inv(D) @ W\n",
    "\n",
    "n_iter = 100\n",
    "x = np.zeros((10,n_iter))\n",
    "\n",
    "# set initial condition (1,0,0,0,0,0,0,0,0,0)\n",
    "x[0,0] = 1\n",
    "\n",
    "# evolve the states\n",
    "for t in range(1,n_iter):\n",
    "    x[:,t] = P @ x[:,t-1]\n",
    "\n",
    "print(\"Average final opinions:\", np.mean(x[:,n_iter-1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can prove that this is equivalent to having node 0 stubborn with opinion 1.\n",
    "\n",
    "Indeed, note that because of the topology of the graph, the opinion of node 0 is not influenced by anyone. The same dynamics can be obtained by assuming that node 0 is stubborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stubborn and regular nodes\n",
    "stubborn = [0];\n",
    "regular = [node for node in G.nodes if node not in stubborn]\n",
    "\n",
    "print(\"Stubborn nodes:\", stubborn, \"\\n\")\n",
    "print(\"Regular nodes:\", regular, \"\\n\")\n",
    "\n",
    "# Input to stubborn nodes\n",
    "u = [1]\n",
    "\n",
    "# Submatrices\n",
    "Q = P[np.ix_(regular, regular)]\n",
    "E = P[np.ix_(regular, stubborn)]\n",
    "\n",
    "# Set the initial condition for the dynamics\n",
    "x = np.zeros((n_nodes,n_iter))\n",
    "x[stubborn,0] = u;\n",
    "print(\"Initial condition:\", x[:,0], \"\\n\")\n",
    "\n",
    "# Evolve the opinion vector\n",
    "for t in range(1,n_iter):\n",
    "    x[regular, t] = Q @ x[regular, t-1] + E @ x[stubborn, t-1]\n",
    "    x[stubborn, t] = x[stubborn, t-1];\n",
    "\n",
    "x_final = x[:,n_iter-1]\n",
    "print(x_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for node in range(G.number_of_nodes()):\n",
    "    trajectory = x[node,:]\n",
    "    ax.plot(trajectory, label='node {0:d}'.format(node))\n",
    "    \n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same trajectory as before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more general model: overview on Friedkin-Johnsen model\n",
    "The French-DeGroot dynamics can be generalized by taking into account that each agents is not completely regular or completely stubborn. The opinion of each agents is in part due to \"innate\" opinions, and in part due to influence of society.\n",
    "\n",
    "The following opinion dynamics model is known as Friedkin-Johnsen model.\n",
    "Here:\n",
    "- $x_i(t)$ is the opinion of the agent $i$;\n",
    "- $y_i$ is the innate opinion of agent $i$.\n",
    "- $\\alpha_i$ is its level of stubborness, i.e., the level of confidence in his/her opinion $y_i$.\n",
    "\n",
    "The dynamics reads:\n",
    "\n",
    "$$\n",
    "x_i(t+1) = \\alpha_i y_i + (1-\\alpha_i) \\sum_{j} P_{ij} x_j(t).\n",
    "$$\n",
    "\n",
    "If $\\alpha = \\mathbf{0}$, we get the French-DeGroot model without input.\n",
    "\n",
    "If $\\alpha \\in \\{0,1\\}^{V}$, we get the French-DeGroot model with stubborn nodes.\n",
    "\n",
    "As the French-DeGroot model with input, also the Friedkin-Johnsen dynamics converges to a non-consensus state, which depends on $\\alpha, y$, but not on the initial opinions $x(0)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
